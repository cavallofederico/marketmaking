{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QzeQrzMu1EPM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\1_Projects\\Tesis Quant UdeSA\\optimal-market-making-main\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "# make the simulation into an RL environment:\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from runstats import *\n",
    "import runstats\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2M5Wrp30Si_",
    "outputId": "e581e98b-3352-435a-9036-4b14a1de1ba4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\1_Projects\\Tesis Quant UdeSA\\optimal-market-making-main\\.venv\\lib\\site-packages\\gym\\spaces\\box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n"
     ]
    }
   ],
   "source": [
    "# MS: Using a discrete action space similar to the Market Making via Reinforcement Learning paper (https://arxiv.org/pdf/1804.04216.pdf, Section 3 - p.3)\n",
    "actions_num = 21   #MS: So the range of possibilities goes from 0.3% to 3% from TOB\n",
    "max_abs_dif = 4\n",
    "max_abs_spread = 20\n",
    "\n",
    "\n",
    "s0 = 100\n",
    "T = 1. # Total time.\n",
    "sigma = 2.  # Standard deviation.\n",
    "dt = .005  # Time step.\n",
    "beta = 0.5\n",
    "kappa = beta * 2\n",
    "k = 1.5\n",
    "A = 137.45\n",
    "\n",
    "def spread(beta, sigma, T_t, k):\n",
    "    return beta*sigma**2*(T_t) + 2/beta*np.log(1+beta/k)\n",
    "\n",
    "def r(beta, sigma, T_t, s, q):\n",
    "    return s - q*beta*sigma**2*(T_t)\n",
    "\n",
    "def l(A, k, d):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "      A : float\n",
    "        in Avellaneda A = \\lambda/\\alpha, where alpha is as above,\n",
    "        and lambda is the constant frequency of market buy and sell orders.\n",
    "      k : float\n",
    "        in Avellaneda k = alpha*K, where alpha ~ 1.5, \n",
    "        and K is such that \\delta p ~ Kln(Q) for a market order of size Q\n",
    "      d : float\n",
    "        in Avellaneda, d=distance to the mid price\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    \n",
    "      l : float:\n",
    "        in Avellaneda, l = lambda = Poisson intensity at which our agentâ€™s orders are\n",
    "        executed.\n",
    "    '''\n",
    "    return A*np.exp(-k*d) \n",
    "    #JK: eq. (12)    \n",
    "\n",
    "\n",
    "class AvellanedaEnv:\n",
    "    def __init__(self, s0, T, dt, sigma, beta, k, A, kappa, seed=0, is_discrete=True):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : float\n",
    "            Initial value of future/stock price.\n",
    "        b : float\n",
    "            Initial value of 'brecha'.\n",
    "        T : float\n",
    "            Total time.\n",
    "        dt : float\n",
    "            Time subdivision.\n",
    "        sigma : float\n",
    "            price volatility.\n",
    "        gamma : float\n",
    "            discount factor.\n",
    "        k : float\n",
    "            in Avellaneda k = alpha*K, where alpha ~ 1.5, \n",
    "            and K is such that \\delta p ~ Kln(Q) for a market order of size Q\n",
    "        A : float\n",
    "            in Avellaneda A = \\lambda/\\alpha, where alpha is as above,\n",
    "            and lambda is the constant frequency of market buy and sell orders.\n",
    "    \n",
    "        '''\n",
    "        self.s0 = s0\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.sigma = sigma\n",
    "        self.beta = beta\n",
    "        self.k = k\n",
    "        self.A = A\n",
    "        self.sqrtdt = np.sqrt(dt)\n",
    "        self.kappa = kappa\n",
    "        self.is_discrete = is_discrete\n",
    "        self.stats = runstats.ExponentialStatistics(decay=0.999)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # observation space: s (price), q, T-t (time remaining)\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0.0, -math.inf, 0.0]),\n",
    "                                     high=np.array([math.inf, math.inf,T]),\n",
    "                                     dtype=np.float32)\n",
    "        # action space: spread, ds\n",
    "        self.action_space = gym.spaces.Discrete(actions_num)\n",
    "        self.reward_range = (-math.inf,math.inf)\n",
    "        \n",
    "        self.metadata = None # useless field\n",
    "        \n",
    "    def reset(self,seed=0):\n",
    "        self.s = self.s0\n",
    "        self.q = 0.0\n",
    "        self.t = 0.0\n",
    "        self.w = 0.0\n",
    "        self.n = int(T/dt)\n",
    "        self.c_ = 0.0\n",
    "        return np.array((self.s,self.q,self.T))\n",
    "        \n",
    "    def step(self, action):\n",
    "        if self.is_discrete:\n",
    "            despl = (action-(actions_num-1)/2)*max_abs_dif/(actions_num-1)\n",
    "        else:\n",
    "            despl = action\n",
    "        ba_spread = spread(self.beta,self.sigma,self.T-self.t,self.k)\n",
    "\n",
    "        bid = self.s - despl - ba_spread/2\n",
    "        ask = self.s - despl + ba_spread/2\n",
    "                \n",
    "        db = self.s - bid\n",
    "        da = ask - self.s\n",
    "        \n",
    "        lb = l(A, k, db)\n",
    "        la = l(A, k, da)\n",
    "        \n",
    "        dnb = 1 if np.random.uniform() <= lb * self.dt else 0\n",
    "        dna = 1 if np.random.uniform() <= la * self.dt else 0\n",
    "        self.q += dnb - dna\n",
    "\n",
    "        self.c_ += -dnb * bid + dna * ask # cash\n",
    "\n",
    "        self.s += self.sigma * self.sqrtdt *(1 if np.random.uniform() < 0.5 else -1)\n",
    "\n",
    "        previous_w = self.w\n",
    "        self.w = self.c_ + self.q * self.s\n",
    "                \n",
    "        dw = (self.w - previous_w)\n",
    "        self.stats.push(dw)\n",
    "        #reward =  np.exp(-self.gamma*previous_w) - np.exp(-self.gamma*self.w) - 1/(self.n)\n",
    "        \n",
    "        #if self.t >= self.T:\n",
    "        reward = dw - self.kappa/2 * (dw - self.stats.mean())**2\n",
    "        \n",
    "        #if self.t >= self.T - self.dt:\n",
    "            #print(\"sum of dw: \" + str(sum(self.ws)))\n",
    "            #print(\"sum of kappa/2 * (dw - mu)**2: \" + str(sum(self.rews)))\n",
    "        \n",
    "        self.t += self.dt\n",
    "\n",
    "            \n",
    "        return np.array((self.s,self.q,self.T-self.t)), reward, self.t >= self.T, {'w':self.w}\n",
    "    \n",
    "env = AvellanedaEnv(s0, T, dt, sigma, beta, k, A,kappa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 25364), started 0:22:18 ago. (Use '!kill 25364' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-70ac15c7bc1757ce\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-70ac15c7bc1757ce\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "ohfqOsj10psv",
    "outputId": "650c7370-1b20-42d5-c23f-4fb073ae6e12",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found! Starting training...\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./logs/DQN_2\n",
      "Eval num_timesteps=500, episode_reward=-7735.71 +/- 786.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.74e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.957     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 620       |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total timesteps  | 900       |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-6760.36 +/- 990.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=-7234.79 +/- 681.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.23e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.91      |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 818       |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total timesteps  | 1900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-6836.71 +/- 927.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2500, episode_reward=-6768.97 +/- 696.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.77e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.862     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 889       |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total timesteps  | 2900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-6554.81 +/- 751.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3500, episode_reward=-7680.69 +/- 635.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.68e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.815     |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 932       |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total timesteps  | 3900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-7597.77 +/- 896.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-7328.34 +/- 837.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.33e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.767     |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 978       |\n",
      "|    time_elapsed     | 5         |\n",
      "|    total timesteps  | 4900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-7395.17 +/- 519.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5500, episode_reward=-7412.73 +/- 1574.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.41e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.72      |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 1004      |\n",
      "|    time_elapsed     | 5         |\n",
      "|    total timesteps  | 5900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-6959.03 +/- 696.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6500, episode_reward=-7230.98 +/- 462.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.23e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.672     |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 1031      |\n",
      "|    time_elapsed     | 6         |\n",
      "|    total timesteps  | 6900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-7096.95 +/- 336.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7500, episode_reward=-7351.91 +/- 755.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.35e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.625     |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 1052      |\n",
      "|    time_elapsed     | 7         |\n",
      "|    total timesteps  | 7900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-7067.78 +/- 1021.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8500, episode_reward=-7412.48 +/- 1098.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.41e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.577     |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 1063      |\n",
      "|    time_elapsed     | 8         |\n",
      "|    total timesteps  | 8900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-7500.15 +/- 848.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9500, episode_reward=-7052.93 +/- 731.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.05e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.53      |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 1077      |\n",
      "|    time_elapsed     | 9         |\n",
      "|    total timesteps  | 9900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-6935.96 +/- 639.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10500, episode_reward=-7037.41 +/- 711.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.04e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.482     |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 1089      |\n",
      "|    time_elapsed     | 10        |\n",
      "|    total timesteps  | 10900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-6761.86 +/- 692.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11500, episode_reward=-6332.56 +/- 419.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.33e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.435     |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 1098      |\n",
      "|    time_elapsed     | 10        |\n",
      "|    total timesteps  | 11900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-7044.29 +/- 711.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12500, episode_reward=-7458.83 +/- 831.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.46e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.387     |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 1101      |\n",
      "|    time_elapsed     | 11        |\n",
      "|    total timesteps  | 12900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-6925.99 +/- 717.94\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=13500, episode_reward=-7236.10 +/- 828.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.24e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.34      |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 1108      |\n",
      "|    time_elapsed     | 12        |\n",
      "|    total timesteps  | 13900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-7185.32 +/- 1033.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=14500, episode_reward=-7068.00 +/- 799.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.07e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.292     |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 1114      |\n",
      "|    time_elapsed     | 13        |\n",
      "|    total timesteps  | 14900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-7001.23 +/- 624.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=15500, episode_reward=-7190.49 +/- 1026.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.19e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.245     |\n",
      "| time/               |           |\n",
      "|    episodes         | 64        |\n",
      "|    fps              | 1120      |\n",
      "|    time_elapsed     | 14        |\n",
      "|    total timesteps  | 15900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-6764.86 +/- 764.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=16500, episode_reward=-6947.46 +/- 1153.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.95e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.197     |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 1123      |\n",
      "|    time_elapsed     | 15        |\n",
      "|    total timesteps  | 16900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-6605.65 +/- 620.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=17500, episode_reward=-7605.81 +/- 831.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.61e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.15      |\n",
      "| time/               |           |\n",
      "|    episodes         | 72        |\n",
      "|    fps              | 1122      |\n",
      "|    time_elapsed     | 15        |\n",
      "|    total timesteps  | 17900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-6791.56 +/- 886.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=18500, episode_reward=-6337.65 +/- 518.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.34e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.102     |\n",
      "| time/               |           |\n",
      "|    episodes         | 76        |\n",
      "|    fps              | 1125      |\n",
      "|    time_elapsed     | 16        |\n",
      "|    total timesteps  | 18900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-6817.56 +/- 1113.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=19500, episode_reward=-7458.66 +/- 526.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.46e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.0547    |\n",
      "| time/               |           |\n",
      "|    episodes         | 80        |\n",
      "|    fps              | 1125      |\n",
      "|    time_elapsed     | 17        |\n",
      "|    total timesteps  | 19900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-6803.42 +/- 611.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=20500, episode_reward=-6386.84 +/- 849.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.39e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 84        |\n",
      "|    fps              | 1125      |\n",
      "|    time_elapsed     | 18        |\n",
      "|    total timesteps  | 20900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-6831.97 +/- 627.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=21500, episode_reward=-6743.34 +/- 912.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.74e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 88        |\n",
      "|    fps              | 1123      |\n",
      "|    time_elapsed     | 19        |\n",
      "|    total timesteps  | 21900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-7068.92 +/- 200.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=22500, episode_reward=-6477.27 +/- 856.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.48e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 92        |\n",
      "|    fps              | 1126      |\n",
      "|    time_elapsed     | 20        |\n",
      "|    total timesteps  | 22900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-7591.47 +/- 777.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=23500, episode_reward=-7488.93 +/- 975.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.49e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 96        |\n",
      "|    fps              | 1129      |\n",
      "|    time_elapsed     | 21        |\n",
      "|    total timesteps  | 23900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-6801.41 +/- 376.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=24500, episode_reward=-7014.88 +/- 881.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.01e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 100       |\n",
      "|    fps              | 1133      |\n",
      "|    time_elapsed     | 21        |\n",
      "|    total timesteps  | 24900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-6161.99 +/- 763.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25500, episode_reward=-6905.64 +/- 796.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.91e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 104       |\n",
      "|    fps              | 1133      |\n",
      "|    time_elapsed     | 22        |\n",
      "|    total timesteps  | 25900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-6700.34 +/- 396.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=26500, episode_reward=-7042.89 +/- 629.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.04e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 108       |\n",
      "|    fps              | 1134      |\n",
      "|    time_elapsed     | 23        |\n",
      "|    total timesteps  | 26900     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=27000, episode_reward=-7606.62 +/- 1084.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=27500, episode_reward=-7562.77 +/- 627.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.56e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 112       |\n",
      "|    fps              | 1132      |\n",
      "|    time_elapsed     | 24        |\n",
      "|    total timesteps  | 27900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-7257.33 +/- 666.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=28500, episode_reward=-6867.85 +/- 851.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.87e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 116       |\n",
      "|    fps              | 1135      |\n",
      "|    time_elapsed     | 25        |\n",
      "|    total timesteps  | 28900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-7284.33 +/- 880.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=29500, episode_reward=-6803.65 +/- 920.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -6.8e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 1136     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total timesteps  | 29900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-6658.68 +/- 1133.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=30500, episode_reward=-7203.27 +/- 572.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.2e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 1137     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total timesteps  | 30900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-7660.81 +/- 685.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=31500, episode_reward=-7285.44 +/- 592.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.29e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 128       |\n",
      "|    fps              | 1140      |\n",
      "|    time_elapsed     | 27        |\n",
      "|    total timesteps  | 31900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-7058.06 +/- 353.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=32500, episode_reward=-6745.71 +/- 658.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.75e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 132       |\n",
      "|    fps              | 1143      |\n",
      "|    time_elapsed     | 28        |\n",
      "|    total timesteps  | 32900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-7893.38 +/- 1017.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=33500, episode_reward=-6967.06 +/- 908.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.97e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 136       |\n",
      "|    fps              | 1144      |\n",
      "|    time_elapsed     | 29        |\n",
      "|    total timesteps  | 33900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-6943.80 +/- 449.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=34500, episode_reward=-6824.78 +/- 787.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.82e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 140       |\n",
      "|    fps              | 1144      |\n",
      "|    time_elapsed     | 30        |\n",
      "|    total timesteps  | 34900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-6958.50 +/- 771.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=35500, episode_reward=-7074.70 +/- 475.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.07e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 144       |\n",
      "|    fps              | 1141      |\n",
      "|    time_elapsed     | 31        |\n",
      "|    total timesteps  | 35900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-6893.48 +/- 882.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=36500, episode_reward=-6858.25 +/- 583.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.86e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 148       |\n",
      "|    fps              | 1140      |\n",
      "|    time_elapsed     | 32        |\n",
      "|    total timesteps  | 36900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-7615.78 +/- 363.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=37500, episode_reward=-7060.70 +/- 811.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.06e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 152       |\n",
      "|    fps              | 1139      |\n",
      "|    time_elapsed     | 33        |\n",
      "|    total timesteps  | 37900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-6486.77 +/- 803.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=38500, episode_reward=-6998.26 +/- 355.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7e+03   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 1141     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total timesteps  | 38900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-7712.90 +/- 548.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=39500, episode_reward=-7089.28 +/- 928.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.09e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 160       |\n",
      "|    fps              | 1141      |\n",
      "|    time_elapsed     | 34        |\n",
      "|    total timesteps  | 39900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-7494.01 +/- 808.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=40500, episode_reward=-7194.17 +/- 973.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.19e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 164       |\n",
      "|    fps              | 1142      |\n",
      "|    time_elapsed     | 35        |\n",
      "|    total timesteps  | 40900     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=41000, episode_reward=-6777.13 +/- 406.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=41500, episode_reward=-7126.24 +/- 680.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.13e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 168       |\n",
      "|    fps              | 1144      |\n",
      "|    time_elapsed     | 36        |\n",
      "|    total timesteps  | 41900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-6903.62 +/- 469.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=42500, episode_reward=-6807.04 +/- 883.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.81e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 172       |\n",
      "|    fps              | 1142      |\n",
      "|    time_elapsed     | 37        |\n",
      "|    total timesteps  | 42900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-6958.43 +/- 711.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=43500, episode_reward=-7189.03 +/- 233.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.19e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 176       |\n",
      "|    fps              | 1143      |\n",
      "|    time_elapsed     | 38        |\n",
      "|    total timesteps  | 43900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-7023.21 +/- 285.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=44500, episode_reward=-6475.08 +/- 308.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.48e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 180       |\n",
      "|    fps              | 1145      |\n",
      "|    time_elapsed     | 39        |\n",
      "|    total timesteps  | 44900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-6939.91 +/- 345.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=45500, episode_reward=-7280.79 +/- 928.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.28e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 184       |\n",
      "|    fps              | 1146      |\n",
      "|    time_elapsed     | 40        |\n",
      "|    total timesteps  | 45900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-6956.21 +/- 536.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=46500, episode_reward=-6905.92 +/- 1013.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.91e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 188       |\n",
      "|    fps              | 1147      |\n",
      "|    time_elapsed     | 40        |\n",
      "|    total timesteps  | 46900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-6904.09 +/- 761.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=47500, episode_reward=-6604.54 +/- 738.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -6.6e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 1146     |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total timesteps  | 47900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-7157.86 +/- 587.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=48500, episode_reward=-7686.65 +/- 772.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.69e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 196       |\n",
      "|    fps              | 1147      |\n",
      "|    time_elapsed     | 42        |\n",
      "|    total timesteps  | 48900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-7053.71 +/- 1233.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=49500, episode_reward=-6560.20 +/- 788.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.56e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 200       |\n",
      "|    fps              | 1146      |\n",
      "|    time_elapsed     | 43        |\n",
      "|    total timesteps  | 49900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-6363.24 +/- 511.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=50500, episode_reward=-5581.29 +/- 967.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -5.58e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 204       |\n",
      "|    fps              | 1124      |\n",
      "|    time_elapsed     | 45        |\n",
      "|    total timesteps  | 50900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 6.78      |\n",
      "|    n_updates        | 224       |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-4417.27 +/- 555.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51500, episode_reward=-302.92 +/- 28.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -303     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 1103     |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total timesteps  | 51900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.43     |\n",
      "|    n_updates        | 474      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-2755.42 +/- 348.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=52500, episode_reward=-246.38 +/- 72.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -246     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 1085     |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total timesteps  | 52900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.81     |\n",
      "|    n_updates        | 724      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-537.20 +/- 55.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=53500, episode_reward=-292.59 +/- 48.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -293     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 1066     |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total timesteps  | 53900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.29     |\n",
      "|    n_updates        | 974      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=54000, episode_reward=6.26 +/- 14.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54500, episode_reward=-156.95 +/- 142.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -157     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 1051     |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total timesteps  | 54900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.59     |\n",
      "|    n_updates        | 1224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-586.12 +/- 50.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=55500, episode_reward=-227.44 +/- 116.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -227     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 1034     |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total timesteps  | 55900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.3      |\n",
      "|    n_updates        | 1474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=15.07 +/- 6.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56500, episode_reward=12.53 +/- 6.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 12.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 1019     |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total timesteps  | 56900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 1724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=27.30 +/- 5.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57500, episode_reward=-17.61 +/- 9.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -17.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 1006     |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total timesteps  | 57900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.795    |\n",
      "|    n_updates        | 1974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-587.74 +/- 44.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=58500, episode_reward=-539.49 +/- 54.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -539     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 992      |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total timesteps  | 58900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.973    |\n",
      "|    n_updates        | 2224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=20.61 +/- 5.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=59500, episode_reward=14.28 +/- 7.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 980      |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total timesteps  | 59900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.559    |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=21.03 +/- 3.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60500, episode_reward=17.67 +/- 5.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 969      |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total timesteps  | 60900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.32     |\n",
      "|    n_updates        | 2724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=14.17 +/- 10.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=61500, episode_reward=13.37 +/- 3.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 956      |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total timesteps  | 61900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.54     |\n",
      "|    n_updates        | 2974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=25.12 +/- 4.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=62500, episode_reward=4.16 +/- 8.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 4.16     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 945      |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total timesteps  | 62900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.44     |\n",
      "|    n_updates        | 3224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=21.68 +/- 6.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=63500, episode_reward=16.36 +/- 7.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 936      |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total timesteps  | 63900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.883    |\n",
      "|    n_updates        | 3474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-58.22 +/- 95.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=64500, episode_reward=-211.61 +/- 260.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -212     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 925      |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total timesteps  | 64900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.875    |\n",
      "|    n_updates        | 3724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=23.15 +/- 6.84\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=65500, episode_reward=-42.93 +/- 87.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -42.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 916      |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total timesteps  | 65900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.857    |\n",
      "|    n_updates        | 3974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=2.03 +/- 10.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=66500, episode_reward=20.53 +/- 8.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 907      |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total timesteps  | 66900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.438    |\n",
      "|    n_updates        | 4224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=12.87 +/- 17.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=67500, episode_reward=24.47 +/- 2.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 899      |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total timesteps  | 67900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.498    |\n",
      "|    n_updates        | 4474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=25.39 +/- 0.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=68500, episode_reward=22.49 +/- 7.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 893      |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total timesteps  | 68900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.544    |\n",
      "|    n_updates        | 4724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-33.53 +/- 13.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=69500, episode_reward=-22.93 +/- 5.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -22.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 886      |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total timesteps  | 69900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.57     |\n",
      "|    n_updates        | 4974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=21.52 +/- 8.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=70500, episode_reward=16.69 +/- 10.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 880      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total timesteps  | 70900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.37     |\n",
      "|    n_updates        | 5224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=25.93 +/- 7.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=71500, episode_reward=25.18 +/- 5.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 874      |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total timesteps  | 71900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.839    |\n",
      "|    n_updates        | 5474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-375.94 +/- 290.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=72500, episode_reward=-46.21 +/- 75.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -46.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 869      |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total timesteps  | 72900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.481    |\n",
      "|    n_updates        | 5724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-4.31 +/- 29.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=73500, episode_reward=22.50 +/- 8.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 863      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total timesteps  | 73900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.479    |\n",
      "|    n_updates        | 5974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=23.32 +/- 4.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=74500, episode_reward=7.73 +/- 15.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.73     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 858      |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total timesteps  | 74900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.538    |\n",
      "|    n_updates        | 6224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=14.14 +/- 4.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=75500, episode_reward=27.47 +/- 3.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 853      |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total timesteps  | 75900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.547    |\n",
      "|    n_updates        | 6474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=0.20 +/- 16.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=76500, episode_reward=14.85 +/- 6.86\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 848      |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total timesteps  | 76900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 6724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=0.85 +/- 37.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=77500, episode_reward=17.89 +/- 9.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 843      |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total timesteps  | 77900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.321    |\n",
      "|    n_updates        | 6974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=21.03 +/- 5.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=78500, episode_reward=-16.45 +/- 35.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -16.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 836      |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total timesteps  | 78900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.421    |\n",
      "|    n_updates        | 7224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-1.31 +/- 41.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=79500, episode_reward=-70.98 +/- 8.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -71      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 830      |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total timesteps  | 79900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.566    |\n",
      "|    n_updates        | 7474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-0.08 +/- 16.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80500, episode_reward=-22.55 +/- 35.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -22.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 826      |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total timesteps  | 80900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.773    |\n",
      "|    n_updates        | 7724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=9.35 +/- 24.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=81500, episode_reward=-27.61 +/- 12.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -27.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 821      |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total timesteps  | 81900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.871    |\n",
      "|    n_updates        | 7974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=21.23 +/- 3.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=82500, episode_reward=17.12 +/- 13.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 816      |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total timesteps  | 82900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.935    |\n",
      "|    n_updates        | 8224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-90.96 +/- 71.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=83500, episode_reward=-120.00 +/- 23.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -120     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 812      |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total timesteps  | 83900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.585    |\n",
      "|    n_updates        | 8474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-4.49 +/- 14.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=84500, episode_reward=-18.16 +/- 88.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -18.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 807      |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total timesteps  | 84900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.592    |\n",
      "|    n_updates        | 8724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=16.01 +/- 14.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=85500, episode_reward=6.32 +/- 12.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.32     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 803      |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total timesteps  | 85900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.595    |\n",
      "|    n_updates        | 8974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-162.04 +/- 314.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=86500, episode_reward=-45.83 +/- 50.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -45.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 799      |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total timesteps  | 86900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.53     |\n",
      "|    n_updates        | 9224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-21.93 +/- 52.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=87500, episode_reward=4.58 +/- 4.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 4.58     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total timesteps  | 87900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.441    |\n",
      "|    n_updates        | 9474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=88000, episode_reward=-129.98 +/- 47.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=88500, episode_reward=-220.60 +/- 163.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -221     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total timesteps  | 88900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.321    |\n",
      "|    n_updates        | 9724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=21.44 +/- 4.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=89500, episode_reward=7.05 +/- 20.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.05     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 787      |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total timesteps  | 89900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.606    |\n",
      "|    n_updates        | 9974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-45.72 +/- 69.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=90500, episode_reward=10.41 +/- 20.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 784      |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total timesteps  | 90900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.579    |\n",
      "|    n_updates        | 10224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-60.63 +/- 122.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=91500, episode_reward=-42.13 +/- 98.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -42.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 780      |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total timesteps  | 91900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.993    |\n",
      "|    n_updates        | 10474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=3.45 +/- 29.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=92500, episode_reward=12.92 +/- 16.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 12.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 778      |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total timesteps  | 92900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.378    |\n",
      "|    n_updates        | 10724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-70.06 +/- 84.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=93500, episode_reward=18.31 +/- 4.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 774      |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total timesteps  | 93900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.33     |\n",
      "|    n_updates        | 10974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=7.10 +/- 29.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=94500, episode_reward=-138.63 +/- 117.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -139     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 772      |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total timesteps  | 94900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.501    |\n",
      "|    n_updates        | 11224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=8.19 +/- 4.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=95500, episode_reward=-41.75 +/- 50.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -41.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 768      |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total timesteps  | 95900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.773    |\n",
      "|    n_updates        | 11474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-963.74 +/- 534.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=96500, episode_reward=-195.61 +/- 79.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -196     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 764      |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total timesteps  | 96900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.29     |\n",
      "|    n_updates        | 11724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-981.88 +/- 51.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=97500, episode_reward=-14.31 +/- 18.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -14.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 761      |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total timesteps  | 97900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.386    |\n",
      "|    n_updates        | 11974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-156.14 +/- 346.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=98500, episode_reward=-7.87 +/- 14.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.87    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 758      |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total timesteps  | 98900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.658    |\n",
      "|    n_updates        | 12224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-6.79 +/- 28.91\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=99500, episode_reward=-13.94 +/- 7.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -13.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 754      |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total timesteps  | 99900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.427    |\n",
      "|    n_updates        | 12474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=8.37 +/- 6.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100500, episode_reward=4.50 +/- 24.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 4.5      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 752      |\n",
      "|    time_elapsed     | 134      |\n",
      "|    total timesteps  | 100900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.869    |\n",
      "|    n_updates        | 12724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-430.40 +/- 348.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=101500, episode_reward=2.92 +/- 10.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 2.92     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 750      |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total timesteps  | 101900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.414    |\n",
      "|    n_updates        | 12974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=5.16 +/- 10.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=102500, episode_reward=9.34 +/- 11.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.34     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 747      |\n",
      "|    time_elapsed     | 137      |\n",
      "|    total timesteps  | 102900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.356    |\n",
      "|    n_updates        | 13224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-320.96 +/- 324.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=103500, episode_reward=-10.95 +/- 30.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -10.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 745      |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total timesteps  | 103900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.99     |\n",
      "|    n_updates        | 13474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=9.44 +/- 15.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=104500, episode_reward=-1.65 +/- 20.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.65    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 743      |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total timesteps  | 104900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.6      |\n",
      "|    n_updates        | 13724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-465.00 +/- 403.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=105500, episode_reward=-186.87 +/- 200.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -187     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 741      |\n",
      "|    time_elapsed     | 142      |\n",
      "|    total timesteps  | 105900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.469    |\n",
      "|    n_updates        | 13974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-198.22 +/- 46.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=106500, episode_reward=13.75 +/- 8.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 740      |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total timesteps  | 106900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.33     |\n",
      "|    n_updates        | 14224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=16.85 +/- 7.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=107500, episode_reward=-552.44 +/- 337.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -552     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 738      |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total timesteps  | 107900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 14474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-286.04 +/- 229.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=108500, episode_reward=-36.05 +/- 68.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -36.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 737      |\n",
      "|    time_elapsed     | 147      |\n",
      "|    total timesteps  | 108900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.433    |\n",
      "|    n_updates        | 14724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-642.92 +/- 105.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=109500, episode_reward=-127.24 +/- 83.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -127     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 735      |\n",
      "|    time_elapsed     | 149      |\n",
      "|    total timesteps  | 109900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 14974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-178.97 +/- 163.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=110500, episode_reward=6.29 +/- 10.58\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.29     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 734      |\n",
      "|    time_elapsed     | 150      |\n",
      "|    total timesteps  | 110900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.842    |\n",
      "|    n_updates        | 15224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-8.41 +/- 47.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=111500, episode_reward=-10.54 +/- 66.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -10.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 732      |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total timesteps  | 111900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.864    |\n",
      "|    n_updates        | 15474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-41.81 +/- 101.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=112500, episode_reward=0.62 +/- 41.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 0.622    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 731      |\n",
      "|    time_elapsed     | 154      |\n",
      "|    total timesteps  | 112900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.528    |\n",
      "|    n_updates        | 15724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=18.19 +/- 5.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=113500, episode_reward=-5.24 +/- 21.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -5.24    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 730      |\n",
      "|    time_elapsed     | 155      |\n",
      "|    total timesteps  | 113900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.2      |\n",
      "|    n_updates        | 15974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-744.80 +/- 359.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=114500, episode_reward=-23.42 +/- 38.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -23.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 729      |\n",
      "|    time_elapsed     | 157      |\n",
      "|    total timesteps  | 114900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.597    |\n",
      "|    n_updates        | 16224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-64.09 +/- 75.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=115500, episode_reward=7.24 +/- 13.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.24     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 727      |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total timesteps  | 115900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.429    |\n",
      "|    n_updates        | 16474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-14.27 +/- 16.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=116500, episode_reward=-18.93 +/- 4.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -18.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 726      |\n",
      "|    time_elapsed     | 160      |\n",
      "|    total timesteps  | 116900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.228    |\n",
      "|    n_updates        | 16724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=14.02 +/- 8.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=117500, episode_reward=-42.71 +/- 87.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -42.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 725      |\n",
      "|    time_elapsed     | 162      |\n",
      "|    total timesteps  | 117900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.797    |\n",
      "|    n_updates        | 16974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-4.77 +/- 9.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=118500, episode_reward=9.36 +/- 12.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.36     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 723      |\n",
      "|    time_elapsed     | 164      |\n",
      "|    total timesteps  | 118900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.309    |\n",
      "|    n_updates        | 17224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-52.52 +/- 79.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=119500, episode_reward=-22.03 +/- 17.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -22      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 722      |\n",
      "|    time_elapsed     | 165      |\n",
      "|    total timesteps  | 119900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.706    |\n",
      "|    n_updates        | 17474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=10.04 +/- 16.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120500, episode_reward=16.09 +/- 5.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 721      |\n",
      "|    time_elapsed     | 167      |\n",
      "|    total timesteps  | 120900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 17724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=12.48 +/- 9.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=121500, episode_reward=6.37 +/- 5.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.37     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 720      |\n",
      "|    time_elapsed     | 169      |\n",
      "|    total timesteps  | 121900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 17974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=122000, episode_reward=-1.35 +/- 7.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=122500, episode_reward=-240.62 +/- 24.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -241     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 719      |\n",
      "|    time_elapsed     | 170      |\n",
      "|    total timesteps  | 122900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.433    |\n",
      "|    n_updates        | 18224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=11.52 +/- 9.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=123500, episode_reward=15.80 +/- 11.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 718      |\n",
      "|    time_elapsed     | 172      |\n",
      "|    total timesteps  | 123900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 18474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-55.44 +/- 108.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=124500, episode_reward=9.10 +/- 9.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.1      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 717      |\n",
      "|    time_elapsed     | 174      |\n",
      "|    total timesteps  | 124900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 18724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=5.52 +/- 23.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=125500, episode_reward=21.42 +/- 6.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 716      |\n",
      "|    time_elapsed     | 175      |\n",
      "|    total timesteps  | 125900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 18974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-98.59 +/- 94.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=126500, episode_reward=13.77 +/- 15.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 714      |\n",
      "|    time_elapsed     | 177      |\n",
      "|    total timesteps  | 126900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.727    |\n",
      "|    n_updates        | 19224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=16.33 +/- 8.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=127500, episode_reward=-297.46 +/- 18.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -297     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 714      |\n",
      "|    time_elapsed     | 179      |\n",
      "|    total timesteps  | 127900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.588    |\n",
      "|    n_updates        | 19474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-23.31 +/- 36.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=128500, episode_reward=-107.37 +/- 88.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -107     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 713      |\n",
      "|    time_elapsed     | 180      |\n",
      "|    total timesteps  | 128900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.256    |\n",
      "|    n_updates        | 19724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-15.64 +/- 54.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=129500, episode_reward=-99.55 +/- 150.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -99.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 712      |\n",
      "|    time_elapsed     | 182      |\n",
      "|    total timesteps  | 129900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 19974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-163.29 +/- 162.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=130500, episode_reward=9.16 +/- 14.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.16     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 711      |\n",
      "|    time_elapsed     | 184      |\n",
      "|    total timesteps  | 130900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.469    |\n",
      "|    n_updates        | 20224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-27.81 +/- 89.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=131500, episode_reward=-197.19 +/- 346.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -197     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 710      |\n",
      "|    time_elapsed     | 185      |\n",
      "|    total timesteps  | 131900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.01     |\n",
      "|    n_updates        | 20474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-159.58 +/- 312.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=132500, episode_reward=20.31 +/- 5.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 709      |\n",
      "|    time_elapsed     | 187      |\n",
      "|    total timesteps  | 132900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.725    |\n",
      "|    n_updates        | 20724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-13.14 +/- 12.08\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=133500, episode_reward=-0.24 +/- 11.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.24    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 708      |\n",
      "|    time_elapsed     | 189      |\n",
      "|    total timesteps  | 133900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.405    |\n",
      "|    n_updates        | 20974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=6.09 +/- 23.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=134500, episode_reward=-3.32 +/- 11.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.32    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 707      |\n",
      "|    time_elapsed     | 190      |\n",
      "|    total timesteps  | 134900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.392    |\n",
      "|    n_updates        | 21224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=15.01 +/- 2.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=135500, episode_reward=10.45 +/- 5.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 706      |\n",
      "|    time_elapsed     | 192      |\n",
      "|    total timesteps  | 135900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 21474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-25.73 +/- 11.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=136500, episode_reward=11.12 +/- 7.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 704      |\n",
      "|    time_elapsed     | 194      |\n",
      "|    total timesteps  | 136900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.302    |\n",
      "|    n_updates        | 21724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=15.05 +/- 10.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=137500, episode_reward=11.69 +/- 7.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 703      |\n",
      "|    time_elapsed     | 195      |\n",
      "|    total timesteps  | 137900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.746    |\n",
      "|    n_updates        | 21974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=11.29 +/- 11.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=138500, episode_reward=-295.66 +/- 79.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -296     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 702      |\n",
      "|    time_elapsed     | 197      |\n",
      "|    total timesteps  | 138900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.751    |\n",
      "|    n_updates        | 22224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-13.13 +/- 10.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=139500, episode_reward=15.84 +/- 5.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 701      |\n",
      "|    time_elapsed     | 199      |\n",
      "|    total timesteps  | 139900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.843    |\n",
      "|    n_updates        | 22474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=15.38 +/- 11.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140500, episode_reward=-1.47 +/- 17.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.47    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 700      |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total timesteps  | 140900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.736    |\n",
      "|    n_updates        | 22724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=4.60 +/- 25.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=141500, episode_reward=17.63 +/- 10.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 699      |\n",
      "|    time_elapsed     | 202      |\n",
      "|    total timesteps  | 141900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.42     |\n",
      "|    n_updates        | 22974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-19.13 +/- 9.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=142500, episode_reward=19.84 +/- 8.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 698      |\n",
      "|    time_elapsed     | 204      |\n",
      "|    total timesteps  | 142900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.809    |\n",
      "|    n_updates        | 23224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-14.16 +/- 34.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=143500, episode_reward=15.54 +/- 10.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 697      |\n",
      "|    time_elapsed     | 206      |\n",
      "|    total timesteps  | 143900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.522    |\n",
      "|    n_updates        | 23474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=0.70 +/- 16.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=144500, episode_reward=-11.43 +/- 25.95\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -11.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 696      |\n",
      "|    time_elapsed     | 208      |\n",
      "|    total timesteps  | 144900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.325    |\n",
      "|    n_updates        | 23724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=14.84 +/- 7.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=145500, episode_reward=23.18 +/- 5.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 695      |\n",
      "|    time_elapsed     | 209      |\n",
      "|    total timesteps  | 145900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.303    |\n",
      "|    n_updates        | 23974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-485.65 +/- 261.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=146500, episode_reward=4.68 +/- 16.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 4.68     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 694      |\n",
      "|    time_elapsed     | 211      |\n",
      "|    total timesteps  | 146900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.392    |\n",
      "|    n_updates        | 24224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=7.12 +/- 4.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=147500, episode_reward=17.73 +/- 9.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 694      |\n",
      "|    time_elapsed     | 213      |\n",
      "|    total timesteps  | 147900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.338    |\n",
      "|    n_updates        | 24474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-166.08 +/- 14.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=148500, episode_reward=21.50 +/- 3.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 693      |\n",
      "|    time_elapsed     | 214      |\n",
      "|    total timesteps  | 148900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.44     |\n",
      "|    n_updates        | 24724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-24.70 +/- 22.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=149500, episode_reward=-258.29 +/- 64.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -258     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 692      |\n",
      "|    time_elapsed     | 216      |\n",
      "|    total timesteps  | 149900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.532    |\n",
      "|    n_updates        | 24974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=11.76 +/- 7.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=150500, episode_reward=8.27 +/- 21.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 691      |\n",
      "|    time_elapsed     | 218      |\n",
      "|    total timesteps  | 150900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.578    |\n",
      "|    n_updates        | 25224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=9.47 +/- 6.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=151500, episode_reward=15.70 +/- 7.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 690      |\n",
      "|    time_elapsed     | 220      |\n",
      "|    total timesteps  | 151900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.588    |\n",
      "|    n_updates        | 25474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-38.46 +/- 114.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=152500, episode_reward=22.01 +/- 2.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22       |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 688      |\n",
      "|    time_elapsed     | 221      |\n",
      "|    total timesteps  | 152900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.441    |\n",
      "|    n_updates        | 25724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-70.99 +/- 116.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=153500, episode_reward=0.29 +/- 8.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 0.294    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 688      |\n",
      "|    time_elapsed     | 223      |\n",
      "|    total timesteps  | 153900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.759    |\n",
      "|    n_updates        | 25974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=13.29 +/- 16.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=154500, episode_reward=9.35 +/- 15.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.35     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 687      |\n",
      "|    time_elapsed     | 225      |\n",
      "|    total timesteps  | 154900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.521    |\n",
      "|    n_updates        | 26224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=16.51 +/- 7.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=155500, episode_reward=7.46 +/- 10.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.46     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 687      |\n",
      "|    time_elapsed     | 226      |\n",
      "|    total timesteps  | 155900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 26474    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=156000, episode_reward=-17.21 +/- 40.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=156500, episode_reward=-2.32 +/- 12.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -2.32    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 228      |\n",
      "|    total timesteps  | 156900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.3      |\n",
      "|    n_updates        | 26724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=13.87 +/- 12.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=157500, episode_reward=15.18 +/- 11.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 230      |\n",
      "|    total timesteps  | 157900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.283    |\n",
      "|    n_updates        | 26974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-13.25 +/- 40.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=158500, episode_reward=18.57 +/- 8.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 685      |\n",
      "|    time_elapsed     | 231      |\n",
      "|    total timesteps  | 158900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.475    |\n",
      "|    n_updates        | 27224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=12.26 +/- 17.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=159500, episode_reward=16.09 +/- 9.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 233      |\n",
      "|    total timesteps  | 159900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.636    |\n",
      "|    n_updates        | 27474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=7.51 +/- 5.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160500, episode_reward=14.29 +/- 7.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 235      |\n",
      "|    total timesteps  | 160900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.799    |\n",
      "|    n_updates        | 27724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=24.75 +/- 2.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=161500, episode_reward=10.72 +/- 17.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 683      |\n",
      "|    time_elapsed     | 236      |\n",
      "|    total timesteps  | 161900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.561    |\n",
      "|    n_updates        | 27974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=19.15 +/- 3.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=162500, episode_reward=17.96 +/- 9.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18       |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 682      |\n",
      "|    time_elapsed     | 238      |\n",
      "|    total timesteps  | 162900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.572    |\n",
      "|    n_updates        | 28224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=13.31 +/- 11.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=163500, episode_reward=7.09 +/- 16.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.09     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 681      |\n",
      "|    time_elapsed     | 240      |\n",
      "|    total timesteps  | 163900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.792    |\n",
      "|    n_updates        | 28474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=3.00 +/- 17.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=164500, episode_reward=-0.54 +/- 13.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.536   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 680      |\n",
      "|    time_elapsed     | 242      |\n",
      "|    total timesteps  | 164900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.29     |\n",
      "|    n_updates        | 28724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=15.25 +/- 3.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=165500, episode_reward=9.69 +/- 15.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.69     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 679      |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total timesteps  | 165900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.259    |\n",
      "|    n_updates        | 28974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=6.38 +/- 13.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=166500, episode_reward=7.13 +/- 25.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.13     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 679      |\n",
      "|    time_elapsed     | 245      |\n",
      "|    total timesteps  | 166900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.219    |\n",
      "|    n_updates        | 29224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-11.98 +/- 61.15\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=167500, episode_reward=-37.94 +/- 59.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -37.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 678      |\n",
      "|    time_elapsed     | 247      |\n",
      "|    total timesteps  | 167900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 29474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=3.08 +/- 26.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=168500, episode_reward=-0.95 +/- 5.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.949   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 677      |\n",
      "|    time_elapsed     | 249      |\n",
      "|    total timesteps  | 168900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.292    |\n",
      "|    n_updates        | 29724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=-90.48 +/- 118.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=169500, episode_reward=-1.81 +/- 26.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.81    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 677      |\n",
      "|    time_elapsed     | 250      |\n",
      "|    total timesteps  | 169900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.698    |\n",
      "|    n_updates        | 29974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-20.78 +/- 78.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=170500, episode_reward=13.30 +/- 21.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 676      |\n",
      "|    time_elapsed     | 252      |\n",
      "|    total timesteps  | 170900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.89     |\n",
      "|    n_updates        | 30224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=20.94 +/- 11.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=171500, episode_reward=13.16 +/- 15.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 676      |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total timesteps  | 171900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.221    |\n",
      "|    n_updates        | 30474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-19.27 +/- 32.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=172500, episode_reward=21.23 +/- 7.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 675      |\n",
      "|    time_elapsed     | 256      |\n",
      "|    total timesteps  | 172900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.317    |\n",
      "|    n_updates        | 30724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=17.53 +/- 12.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=173500, episode_reward=6.11 +/- 15.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.11     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 674      |\n",
      "|    time_elapsed     | 257      |\n",
      "|    total timesteps  | 173900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.304    |\n",
      "|    n_updates        | 30974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-33.82 +/- 31.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=174500, episode_reward=-5.16 +/- 53.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -5.16    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 674      |\n",
      "|    time_elapsed     | 259      |\n",
      "|    total timesteps  | 174900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.612    |\n",
      "|    n_updates        | 31224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=12.36 +/- 9.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=175500, episode_reward=-2.53 +/- 18.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -2.53    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 673      |\n",
      "|    time_elapsed     | 261      |\n",
      "|    total timesteps  | 175900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.383    |\n",
      "|    n_updates        | 31474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=26.57 +/- 5.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=176500, episode_reward=-30.05 +/- 56.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -30.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 673      |\n",
      "|    time_elapsed     | 262      |\n",
      "|    total timesteps  | 176900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.243    |\n",
      "|    n_updates        | 31724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-0.42 +/- 27.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=177500, episode_reward=-351.03 +/- 41.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -351     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 672      |\n",
      "|    time_elapsed     | 264      |\n",
      "|    total timesteps  | 177900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.352    |\n",
      "|    n_updates        | 31974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-26.10 +/- 47.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=178500, episode_reward=-11.02 +/- 38.23\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -11      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 671      |\n",
      "|    time_elapsed     | 266      |\n",
      "|    total timesteps  | 178900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.313    |\n",
      "|    n_updates        | 32224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=13.18 +/- 5.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=179500, episode_reward=-1.97 +/- 37.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.97    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 670      |\n",
      "|    time_elapsed     | 268      |\n",
      "|    total timesteps  | 179900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.286    |\n",
      "|    n_updates        | 32474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=9.96 +/- 24.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=180500, episode_reward=-0.63 +/- 30.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.635   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 670      |\n",
      "|    time_elapsed     | 269      |\n",
      "|    total timesteps  | 180900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.967    |\n",
      "|    n_updates        | 32724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=2.92 +/- 10.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=181500, episode_reward=23.26 +/- 10.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 669      |\n",
      "|    time_elapsed     | 271      |\n",
      "|    total timesteps  | 181900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.591    |\n",
      "|    n_updates        | 32974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-12.63 +/- 17.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=182500, episode_reward=-6.52 +/- 11.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -6.52    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 669      |\n",
      "|    time_elapsed     | 273      |\n",
      "|    total timesteps  | 182900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.763    |\n",
      "|    n_updates        | 33224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=8.23 +/- 11.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=183500, episode_reward=-9.16 +/- 25.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -9.16    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 668      |\n",
      "|    time_elapsed     | 274      |\n",
      "|    total timesteps  | 183900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.324    |\n",
      "|    n_updates        | 33474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-6.35 +/- 6.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=184500, episode_reward=-7.84 +/- 37.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.84    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 668      |\n",
      "|    time_elapsed     | 276      |\n",
      "|    total timesteps  | 184900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.614    |\n",
      "|    n_updates        | 33724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=12.70 +/- 5.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=185500, episode_reward=8.37 +/- 34.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.37     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 668      |\n",
      "|    time_elapsed     | 278      |\n",
      "|    total timesteps  | 185900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.334    |\n",
      "|    n_updates        | 33974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-9.30 +/- 17.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=186500, episode_reward=-3.22 +/- 41.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.22    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 667      |\n",
      "|    time_elapsed     | 279      |\n",
      "|    total timesteps  | 186900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.735    |\n",
      "|    n_updates        | 34224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-10.60 +/- 13.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=187500, episode_reward=6.51 +/- 21.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.51     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 667      |\n",
      "|    time_elapsed     | 281      |\n",
      "|    total timesteps  | 187900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.314    |\n",
      "|    n_updates        | 34474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=15.02 +/- 11.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=188500, episode_reward=7.57 +/- 18.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 666      |\n",
      "|    time_elapsed     | 283      |\n",
      "|    total timesteps  | 188900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 34724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=9.63 +/- 8.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=189500, episode_reward=22.18 +/- 7.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 666      |\n",
      "|    time_elapsed     | 284      |\n",
      "|    total timesteps  | 189900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.693    |\n",
      "|    n_updates        | 34974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=190000, episode_reward=21.81 +/- 7.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=190500, episode_reward=26.81 +/- 6.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 666      |\n",
      "|    time_elapsed     | 286      |\n",
      "|    total timesteps  | 190900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.55     |\n",
      "|    n_updates        | 35224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=8.79 +/- 24.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=191500, episode_reward=-131.37 +/- 46.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -131     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 665      |\n",
      "|    time_elapsed     | 288      |\n",
      "|    total timesteps  | 191900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.264    |\n",
      "|    n_updates        | 35474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=18.54 +/- 6.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=192500, episode_reward=8.57 +/- 11.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 665      |\n",
      "|    time_elapsed     | 289      |\n",
      "|    total timesteps  | 192900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.185    |\n",
      "|    n_updates        | 35724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=-29.31 +/- 27.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=193500, episode_reward=2.35 +/- 21.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 2.35     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 665      |\n",
      "|    time_elapsed     | 291      |\n",
      "|    total timesteps  | 193900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.377    |\n",
      "|    n_updates        | 35974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=9.88 +/- 16.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=194500, episode_reward=-2.82 +/- 6.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -2.82    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 664      |\n",
      "|    time_elapsed     | 293      |\n",
      "|    total timesteps  | 194900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 36224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-8.99 +/- 26.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=195500, episode_reward=20.72 +/- 5.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 664      |\n",
      "|    time_elapsed     | 294      |\n",
      "|    total timesteps  | 195900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.348    |\n",
      "|    n_updates        | 36474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=12.87 +/- 4.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=196500, episode_reward=-28.96 +/- 43.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -29      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 664      |\n",
      "|    time_elapsed     | 296      |\n",
      "|    total timesteps  | 196900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.327    |\n",
      "|    n_updates        | 36724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=10.10 +/- 14.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=197500, episode_reward=-98.96 +/- 14.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -99      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 663      |\n",
      "|    time_elapsed     | 298      |\n",
      "|    total timesteps  | 197900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.594    |\n",
      "|    n_updates        | 36974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=15.87 +/- 6.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=198500, episode_reward=-14.58 +/- 6.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -14.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 663      |\n",
      "|    time_elapsed     | 299      |\n",
      "|    total timesteps  | 198900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.331    |\n",
      "|    n_updates        | 37224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-8.83 +/- 11.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=199500, episode_reward=-44.55 +/- 10.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -44.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 663      |\n",
      "|    time_elapsed     | 301      |\n",
      "|    total timesteps  | 199900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.857    |\n",
      "|    n_updates        | 37474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=12.29 +/- 17.54\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fd3c6035e90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "\n",
    "print(\"Model not found! Starting training...\")\n",
    "policy_kwargs = dict(net_arch=[10,10])\n",
    "model = DQN('MlpPolicy', env, policy_kwargs=policy_kwargs, verbose=1, gamma=1.0, tensorboard_log=\"./logs/\")\n",
    "total_timesteps = 200000\n",
    "model.learn(total_timesteps=total_timesteps,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "\n",
    "# Load best model!\n",
    "model = DQN.load(\"./logs/best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-PRu_fXD0ENx"
   },
   "outputs": [],
   "source": [
    "# optimal policy agent as per Avellaneda\n",
    "\n",
    "def spread_func(beta, sigma, k):\n",
    "    return lambda T_t: spread(beta, sigma, T_t, k) \n",
    "\n",
    "def r_func(sigma, beta):\n",
    "    return lambda T_t, s, q: r(beta, sigma, T_t, s, q)\n",
    "    \n",
    "class AvellanedaAgent:\n",
    "    def __init__(self, beta, sigma, k):\n",
    "        self.spread_func = spread_func(beta, sigma, k)\n",
    "        self.r_func = r_func(sigma, beta)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        spread = self.spread_func(observation[2])\n",
    "        r_ = self.r_func(observation[2], observation[0], observation[1])\n",
    "        \n",
    "        bid = r_ - spread/2\n",
    "        ask = r_ + spread/2\n",
    "\n",
    "        ds = observation[0] - r_\n",
    "        \n",
    "        #return spread, ds\n",
    "        return ds\n",
    "\n",
    "    def step(self,observation):\n",
    "        return self.act(observation)\n",
    "\n",
    "#agent = AvellanedaAgent(gamma, sigma, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "S7FXF8L50K2V"
   },
   "outputs": [],
   "source": [
    "# symmetrical policy agent as per Avellaneda\n",
    "\n",
    "class SymmetricAgent:\n",
    "    def __init__(self, beta, sigma, k):\n",
    "        self.spread_func = spread_func(beta, sigma, k)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        #spread = self.spread_func(observation[2])\n",
    "        return 0\n",
    "\n",
    "    def step(self,observation):\n",
    "        return self.act(observation)\n",
    "\n",
    "#symmetric_agent = SymmetricAgent(gamma, sigma, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_GGrBbZWz7pK"
   },
   "outputs": [],
   "source": [
    "def run_env_agent_comp(envs, agent_rl,agent_opt,agent_sym):\n",
    "    \n",
    "    env = envs[0]\n",
    "    \n",
    "    obs = env.reset()\n",
    "    bids_rl = np.zeros(env.n)\n",
    "    asks_rl = np.zeros(env.n)\n",
    "    ss_rl = np.zeros(env.n)\n",
    "    ws_rl = np.zeros(env.n)\n",
    "    qs_rl = np.zeros(env.n)\n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_rl = 0.0\n",
    "    while not final:\n",
    "\n",
    "        action_rl = agent_rl.predict(obs,deterministic=True)\n",
    "        ss_rl[i] = obs[0]\n",
    "        qs_rl[i] = obs[1]\n",
    "        \n",
    "        despl = (action_rl[0]-(actions_num-1)/2)*max_abs_dif/(actions_num-1)\n",
    "        ba_spread = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "\n",
    "        bids_rl[i] = ss_rl[i] - despl - ba_spread/2\n",
    "        asks_rl[i] = ss_rl[i] - despl + ba_spread/2\n",
    "\n",
    "        obs, reward, final, w_rl = env.step(action_rl[0])\n",
    "        i += 1\n",
    "        total_reward_rl += reward\n",
    "\n",
    "      \n",
    "    \n",
    "\n",
    "    env = envs[1]\n",
    "    \n",
    "    obs = env.reset()\n",
    "    bids_opt = np.zeros(env.n)\n",
    "    asks_opt = np.zeros(env.n)\n",
    "    ds_opt = np.zeros(env.n)\n",
    "    spread_opt = np.zeros(env.n)\n",
    "    ss_opt = np.zeros(env.n)\n",
    "    ws_opt = np.zeros(env.n)\n",
    "    qs_opt = np.zeros(env.n)\n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_opt = 0.0\n",
    "    while not final:\n",
    "        action_opt = agent_opt.step(obs)\n",
    "\n",
    "        ds_opt[i] = action_opt\n",
    "        spread_opt[i] = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "        \n",
    "        ss_opt[i] = obs[0]\n",
    "        qs_opt[i] = obs[1]\n",
    "\n",
    "        bids_opt[i] = ss_opt[i] - ds_opt[i] - spread_opt[i]/2\n",
    "        asks_opt[i] = ss_opt[i] - ds_opt[i] + spread_opt[i]/2\n",
    "\n",
    "        obs, reward, final, w_opt = env.step(action_opt)\n",
    "        total_reward_opt += reward\n",
    "        i += 1\n",
    "\n",
    "    env = envs[2]\n",
    "\n",
    "    obs = env.reset()\n",
    "    bids_sym = np.zeros(env.n)\n",
    "    asks_sym = np.zeros(env.n)\n",
    "    ds_sym = np.zeros(env.n)\n",
    "    spread_sym = np.zeros(env.n)\n",
    "    ss_sym = np.zeros(env.n)\n",
    "    ws_sym = np.zeros(env.n)\n",
    "    qs_sym = np.zeros(env.n)    \n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_sym = 0.0\n",
    "    while not final:\n",
    "        action_sym = agent_sym.step(obs)\n",
    "\n",
    "        ds_sym[i] = action_sym\n",
    "        spread_sym[i] = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "        \n",
    "        ss_sym[i] = obs[0]\n",
    "        qs_sym[i] = obs[1]\n",
    "\n",
    "        bids_sym[i] = ss_sym[i] - ds_sym[i] - spread_sym[i]/2\n",
    "        asks_sym[i] = ss_sym[i] - ds_sym[i] + spread_sym[i]/2\n",
    "        \n",
    "        obs, reward, final, w_sym = env.step(action_sym)\n",
    "        i += 1\n",
    "        total_reward_sym += reward\n",
    "\n",
    "        \n",
    "    return w_rl['w'], w_opt['w'], w_sym['w'],total_reward_rl,total_reward_opt,total_reward_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pgV6nOhgAupx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "1.0%\n",
      "2.0%\n",
      "3.0%\n",
      "4.0%\n",
      "5.0%\n",
      "6.0%\n",
      "7.0%\n",
      "8.0%\n",
      "9.0%\n",
      "10.0%\n",
      "11.0%\n",
      "12.0%\n",
      "13.0%\n",
      "14.0%\n",
      "15.0%\n",
      "16.0%\n",
      "17.0%\n",
      "18.0%\n",
      "19.0%\n",
      "20.0%\n",
      "21.0%\n",
      "22.0%\n",
      "23.0%\n",
      "24.0%\n",
      "25.0%\n",
      "26.0%\n",
      "27.0%\n",
      "28.0%\n",
      "29.0%\n",
      "30.0%\n",
      "31.0%\n",
      "32.0%\n",
      "33.0%\n",
      "34.0%\n",
      "35.0%\n",
      "36.0%\n",
      "37.0%\n",
      "38.0%\n",
      "39.0%\n",
      "40.0%\n",
      "41.0%\n",
      "42.0%\n",
      "43.0%\n",
      "44.0%\n",
      "45.0%\n",
      "46.0%\n",
      "47.0%\n",
      "48.0%\n",
      "49.0%\n",
      "50.0%\n",
      "51.0%\n",
      "52.0%\n",
      "53.0%\n",
      "54.0%\n",
      "55.0%\n",
      "56.0%\n",
      "57.0%\n",
      "58.0%\n",
      "59.0%\n",
      "60.0%\n",
      "61.0%\n",
      "62.0%\n",
      "63.0%\n",
      "64.0%\n",
      "65.0%\n",
      "66.0%\n",
      "67.0%\n",
      "68.0%\n",
      "69.0%\n",
      "70.0%\n",
      "71.0%\n",
      "72.0%\n",
      "73.0%\n",
      "74.0%\n",
      "75.0%\n",
      "76.0%\n",
      "77.0%\n",
      "78.0%\n",
      "79.0%\n",
      "80.0%\n",
      "81.0%\n",
      "82.0%\n",
      "83.0%\n",
      "84.0%\n",
      "85.0%\n",
      "86.0%\n",
      "87.0%\n",
      "88.0%\n",
      "89.0%\n",
      "90.0%\n",
      "91.0%\n",
      "92.0%\n",
      "93.0%\n",
      "94.0%\n",
      "95.0%\n",
      "96.0%\n",
      "97.0%\n",
      "98.0%\n",
      "99.0%\n"
     ]
    }
   ],
   "source": [
    "number_of_sims = 1000\n",
    "\n",
    "n = int(T/dt)\n",
    "ws_rl = np.zeros(number_of_sims)\n",
    "ws_opt = np.zeros(number_of_sims)\n",
    "ws_sym = np.zeros(number_of_sims)\n",
    "tr_rl = np.zeros(number_of_sims)\n",
    "tr_opt = np.zeros(number_of_sims)\n",
    "tr_sym = np.zeros(number_of_sims)\n",
    "\n",
    "envs = [AvellanedaEnv(s0, T, dt, sigma, beta, k, A, kappa),AvellanedaEnv(s0, T, dt, sigma, beta, k, A,kappa, seed=0, is_discrete=False),AvellanedaEnv(s0, T, dt, sigma, beta, k, A,kappa, seed=0, is_discrete=False)]\n",
    "for i in range(number_of_sims):\n",
    "    if i%10 == 0:\n",
    "        print(str(i/10) + \"%\")\n",
    "    ws_rl[i], ws_opt[i], ws_sym[i], tr_rl[i], tr_opt[i], tr_sym[i] = run_env_agent_comp(envs, model,AvellanedaAgent(beta, sigma, k),SymmetricAgent(beta, sigma, k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Byg9c4wIC22a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accumulated wealth histogram')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure 2 (p. 222)\n",
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([ws_opt,ws_rl], bins=30,edgecolor='black', label=['Optimum','RL Discrete'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Wealth\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Accumulated wealth histogram\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5QZDlL5dN3DX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accumulated wealth histogram')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([tr_opt,tr_sym,tr_rl], bins=30,edgecolor='black', label=['Optimum', 'Symmetric','RL Discrete'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Total Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Accumulated wealth histogram\")\n",
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([tr_opt,tr_rl], bins=30,edgecolor='black', label=['Optimum','RL Discrete'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Total Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Accumulated wealth histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LUl_ON_bDS1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimo:\n",
      "47.79761695404737\n",
      "6.099978058681764\n",
      "7.83570309503321\n",
      "Simetrico:\n",
      "57.676984427753716\n",
      "11.86348257940186\n",
      "4.8617245435076715\n",
      "RL:\n",
      "53.47532768123994\n",
      "6.681371589804759\n",
      "8.003645204053466\n",
      "\n",
      "Optimum utility function value: \t-2.6283680458275698e-09\n",
      "Symmetric utility function value: \t-4.34167929750502e-06\n",
      "RL utility function value: \t\t-2.20782093515251e-10\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimo:\")\n",
    "print(np.mean(ws_opt))\n",
    "print(np.std(ws_opt))\n",
    "print(np.mean(ws_opt)/np.std(ws_opt))\n",
    "print(\"Simetrico:\")\n",
    "print(np.mean(ws_sym))\n",
    "print(np.std(ws_sym))\n",
    "print(np.mean(ws_sym)/np.std(ws_sym))\n",
    "print(\"RL:\")\n",
    "print(np.mean(ws_rl))\n",
    "print(np.std(ws_rl))\n",
    "print(np.mean(ws_rl)/np.std(ws_rl))\n",
    "\n",
    "print()\n",
    "\n",
    "utility_avellaneda = np.mean(-np.exp(-beta*ws_opt))\n",
    "utility_rl = np.mean(-np.exp(-beta*ws_rl))\n",
    "\n",
    "print(\"Optimum utility function value: \\t{}\".format(utility_avellaneda))\n",
    "print(\"Symmetric utility function value: \\t{}\".format(np.mean(-np.exp(-beta*ws_sym))))\n",
    "print(\"RL utility function value: \\t\\t{}\".format(utility_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UJqDqpVIeGti"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimo:\n",
      "22.46050972128678\n",
      "3.4646472377108304\n",
      "6.482769580930536\n",
      "Simetrico:\n",
      "-7.172303914746957\n",
      "42.1491264960118\n",
      "-0.1701649479123988\n",
      "RL:\n",
      "29.053545034292075\n",
      "4.690815354669816\n",
      "6.193708947713875\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimo:\")\n",
    "print(np.mean(tr_opt))\n",
    "print(np.std(tr_opt))\n",
    "print(np.mean(tr_opt)/np.std(tr_opt))\n",
    "\n",
    "print(\"Simetrico:\")\n",
    "print(np.mean(tr_sym))\n",
    "print(np.std(tr_sym))\n",
    "print(np.mean(tr_sym)/np.std(tr_sym))\n",
    "\n",
    "print(\"RL:\")\n",
    "print(np.mean(tr_rl))\n",
    "print(np.std(tr_rl))\n",
    "print(np.mean(tr_rl)/np.std(tr_rl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('out_ws.csv', 'w') as f: \n",
    "      \n",
    "    # using csv.writer method from CSV package \n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['DQN', 'OPT','SYM']) \n",
    "    for index, _ in enumerate(ws_rl):\n",
    "        write.writerow([ws_rl[index], ws_opt[index],ws_sym[index]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('out_tr.csv', 'w') as f: \n",
    "      \n",
    "    # using csv.writer method from CSV package \n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['DQN', 'OPT','SYM']) \n",
    "    for index, _ in enumerate(tr_rl):\n",
    "        write.writerow([tr_rl[index], tr_opt[index],tr_sym[index]]) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sim_corregida.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8a6dab81d9fff2b9ae223102115e7829ca4b16ccb60ed9eceb6088c7e3c6963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
