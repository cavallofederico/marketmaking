%***********************************************************************************************************
%*****************************************************************************PACKAGES*********************
%Paquetes para espa�ol y matem�tica
%Paquetes para incluir acentos
%Paquetes para incluir graficos
%para incluir c�digos de matlab
%***********************************************************************************************************
%\input{tcilatex}

\documentclass[12pt,a4paper,spanish]{article}%
\usepackage[affil-it]{authblk}
\usepackage{amsmath,amsbsy,amscd,amssymb,graphicx,epsfig,makeidx,multicol}
\usepackage[round]{natbib}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{setspace}
\usepackage[spanish]{babel}
\usepackage[latin1]{inputenc}
%\usepackage[sort&compress]{natbib}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
%\usepackage{biblatex} 
\usepackage{cases}
\usepackage{graphicx,subcaption}
\usepackage{listings}
\usepackage{color}%
\usepackage{amsmath}%
%\usepackage{bbm}
\setcounter{MaxMatrixCols}{30}%
\usepackage{amsfonts}%
\usepackage{dsfont}%
\usepackage{amssymb}%
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{floatrow}
\usepackage{caption}
\usepackage[titletoc,toc]{appendix}
%\usepackage[title]{appendix}
%\usepackage{epstopdf}
%\usepackage{epsfig}
\usepackage[section]{placeins}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{verbatim}
\usepackage[toc]{glossaries}
\usepackage{tocbibind}
%EndMSIPreambleData
\newcommand{\tb}[1]{\textcolor{blue}{#1}}
\definecolor{dkgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{   language=Matlab,                  basicstyle=\footnotesize,             keywordstyle=\color{blue},            commentstyle=\color{dkgreen},         stringstyle=\color{mauve},           escapeinside={\%*}{*)},                tabsize=2
}
\renewcommand{\appendixpagename}{Ap�ndices}
\renewcommand{\appendixtocname}{Ap�ndices}
\renewcommand{\appendixname}{Ap�ndices}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}

%Upper part of the page
\includegraphics[width=0.3\textwidth]{LogoUDESA} \\[2cm]    

\begin{center}

\textsc{\LARGE Universidad de San Andr\'{e}s}\\[1.0cm]

\textsc{\Large Propuesta de Tesis de Maestr\'{i}a en Finanzas}
\\[2.5cm]


% Title
%\HRule \\[0.4cm]
\doublespacing

{ \Large \bfseries \textit{Market making} con se�ales alfa en mercados emergentes}\\[0.4cm]

\vspace{4cm}

\bigskip
\bigskip
\begin{singlespace}

% Author and supervisor
\begin{minipage}{0.45\textwidth}
\begin{flushleft} \large
\emph{Autor:}\\
Federico Cavallo
\end{flushleft}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{flushright} \large
\emph{Tutor:} \\
Javier Kreiner

\emph{Co-Tutor:} \\
Gabriel Basaluzzo
\end{flushright}
\end{minipage}

\end{singlespace}

\vfill

% Bottom of the page
{\large Febrero de 2023}

\end{center}

\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plainnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents
\thispagestyle{empty}

\newpage

%{\thispagestyle{empty}} %DON`T DELETE THIS LINE

\pagenumbering{arabic}

\listoffigures

%\listoftables

\section*{Glosario}
\begin{description}
	\item[BOVESPA] Bolsa de Valores del Estado de San Pablo
	\item[NASDAQ] Bolsa de Valores de Nueva York
	\item[\textit{PnL}] \textit{Profit and Loss} o retorno
\end{description}

\hfill \break

\noindent {\bf Resumen}
Se analiza el problema de un agente de \textit{market making} para el caso de un mercado electr�nico de alta frecuencia en el tope del libro de �rdenes. Se intenta probar que un modelo �ptimo de programaci�n din�mica aplicado a \textit{market making} obtenido de la literatura logra resultados de retorno positivos frente a una estrategia base en un entorno de simulaci�n con datos creados artificialmente en base a par�metros de mercados emergentes. Otro objetivo de este trabajo es desarrollar un simulador que permita probar esta estrategia frente a datos de la realidad para dichos mercados. Se explica dicho modelo y c�mo se pueden estimar los par�metros del mercado. Se presentan resultados preeliminares exitosos de la replicaci�n del modelo. Se presentan los pasos a seguir para concluir con la investigaci�n.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introducci�n}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Contexto del proyecto o Intro de la intro: algo de MM
En los mercados electr�nicos modernos, donde se intercambian activos a velocidades de milisegundos, surge la problem�tica de la falta de liquidez o de contraparte generando a su vez el problema de faltante de precio y la consecuente necesidad de realizar una b�squeda de precio que determine cu�l es el precio justo para un activo. Por esta raz�n, surgen actores que vienen a suplir esta necesidad ofreciendo liquidez de forma permanente. Es decir, ofrecen una punta vendedora y una punta compradora de forma simult�nea y a lo largo del tiempo. Estos agentes son llamados \textit{market makers} o Creadores de Mercado. Estos participantes, en muchos casos, tienen acuerdos con el mercado quie los incentiva a tener este comportamiento de proveer liquididez. En cualquier caso deber�n hacerlo de forma tal que el rendimiento sea positivo, si no, no podr�n mantenerse en el mercado.

En este contexto existen diferentes tipos de estrategias que pueden tomar estos agentes para decidir c�mo ofrecer la liquidez al mercado de forma redituable. Esto depender� del mercado, sus caracter�sticas, del modelo utilizado para analizar el problema y de los algoritmos elegidos para solucionarlo. Entre esas categor�as se encuentran los algoritmos de programaci�n din�mica donde se busca obtener una estrategia �ptima que permita maximizar el resultado de una funci�n de utilidad a lo largo del tiempo. Por otro lado, en los mercados de alta frecuencia se puede generar lo que se llama una se�al alfa que consiste en un desbalance moment�neo entre la oferta y la demanda de �rdenes de compra o venta que permitir�a inferir en que direcci�n se va a mover el mercado en el cort�simo plazo. En general, estas estrategias han sido testeadas en mercados desarrollados que tienen particularidades y diferencias respecto a los mercados emergentes.

\cite{Cartea2019} hacen uso de programaci�n din�mica para desarrollar un algoritmo que permite ofrecer liquidez vali�ndose de la se�al alfa de forma tal de generar un mejor rendimiento que una estrategia de base. Analizan los par�metros del NASDAQ y realizan una simulaci�n contra un escenario base. 

En el presente trabajo se replican, en primer lugar, replicar los resultados obtenidos por \cite{Cartea2019}, implementando su algoritmo en base a los datos publicados y realizando una serie de simulaciones. Esto con la intenci�n de luego tomar los datos de un activo de alta liquidez del mercado brasile�o BOVESPA, estimar sus par�metros y responder la pregunta de si este modelo otorga retornos positivos contra un algoritmo de referencia en un mercado emergente como el brasile�o. Finalmente, se dise�ar� un simulador que permita probar esta estrategia contra datos reales y ya no una simulaci�n con par�metros obtenidos a partir de los datos de mercado. Con esto se intentar� responder una segunda pregunta referente a si este modelo es capaz de entregar resultados de retorno superiores a una estrategia de base contra los datos reales. Se espera que ambas respuestas otorgen resultados positivos, dado que si bien se trata de un mercado emergente estamos frente a uno con una liquidez muy alta.

En la Secci�n \ref{sec:revision} se hace una revisi�n de la bibliograf�a reelevante particularmente de programaci�n din�mica y en menor medida de aprendizaje reforzado. En la Secci�n \ref{sec:problema} se define el problema de \textit{market making}. En la secci�n \ref{sec:modelo} se hace una descripci�n pormenorizada del modelo utilizado para la obtenci�n de los resultados y se presenta c�mo se pueden obtener par�metros de mercado. En la secci�n \ref{sec:metodo} se describe la metodolog�a empleada para realizar las simulaciones. En la secci�n \ref{sec:resultados} se presentan los resultados preelminares del trabajo. Finalmente, en la Secci�n \ref{sec: futuro} se delinea el plan de trabajo futuro para concluir la tesis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section {Revisi�n literaria}\label{sec:revision}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
En la literatura hay diferentes vertientes para solucionar el problema de ofrecer permanentemente liquidez a un mercado, denominado \textit{market making}, as� como para realizar \textit{trading} de alta frecuencia.

\subsection{Programaci�n din�mica}
%Introducci�n

%Ho and Stoll - Trabajo pionero en el tema.
\cite{Ho1981} analizan el caso de un agente �nico que tiene una demanda estoc�stica de �rdenes de compra y venta. Utilizan programaci�n din�mica para obtener precios de compra y de venta �ptimos maximizando una funci�n de utilidad y riqueza terminal teniendo en cuenta el inventario.

%%% Hito Avellaneda
% Que hicieron? bid ask, limit order book, poisson, benchmark, pnl mejor que benchmark, varias estrategias, simulaciones perfil de pnl
\cite{Avellaneda2008} analizan la microestructura de mercado al estudiar el problema de \textit{market making}. 
% Explicitan como definir \textit{bid} y \textit{ask} �ptimos cuando las ordenes de compra y venta de mercado siguen un proceso de arribos de Poisson. 
Definen dos estrategias m�s sencillas (sim�trica y mejor compra/mejor venta) como referencia mejorarando el perfil de retornos comparativamente frente a ellas en una serie de simulaciones.
% probabilidad de ser agredidos en funci�n de la distancia
A su vez, modelizan la probabilidad de ser agredidos en un libro de �rdenes l�mite en funci�n de la distancia al precio medio en que definen las �rdenes l�mite. Esto resulta �til desde el punto de vista te�rico, pero plantea una limitaci�n en la pr�ctica para mercados con alta liquidez donde solamente ser�n agredidas las �rdenes de compra/venta que est�n en el tope. Para sobrepasar este problema se necesita de un modelo que decida de forma binaria si ofrecer una �rden de compra y/o de venta en el tope a cada instante de tiempo.
% Fortaleza riesgo de inventario, falta de riesgo asimetr�a de la info.
Por otro lado, su modelo toma en consideraci�n el riesgo de inventario mediante la definici�n de un precio de indiferencia que se acerca al precio medio a medida que se termina la sesi�n de mercado; y una funci�n de utilidad que penaliza cargar el inventario a lo largo del tiempo, pero adolece de consideraciones respecto a la asimetr�a de la informaci�n que podr�an ser consideradas en una funci�n de utilidad m�s compleja. Tampoco considera la posibilidad de descargar el inventario mediante �rdenes de mercado.
% Proceso browniano de precios, no considera noticias ni traders informados.
En cuanto a su modelo, definen un proceso de precios Browniano que replica el comportamiento te�rico del activo subyacente pero que deja de lado el arribo de noticias novedosas, no contempla el comportamiento de agentes informados, ni tampoco consideara que los precios se mueven en una grilla discreta. Lo que podr�a ser solucionado con un proceso de saltos como proceso de precios y la inclusi�n de \textit{shocks} estoc�sticos simulando el arribo de noticias.
%funci�n de utilidad HJB programaci�n din�mica
Para optimizar la funci�n de utilidad se valen de la ecuaci�n de Hamilton Jacobi Bellman y su uso en programaci�n din�mica.
%modelado de precios en base al nasdaq
Tanto el proceso de precios como las intensidades de Poisson son definidas en base a par�metros del NASDAQ, dejando abierto el interrogante de su aplicabilidad en mercados emergentes como el latinoamericano.

%% Cartea Robustez
\cite{Cartea2013} plantean un modelo robusto a las especificaciones incorrectas del modelo de precios, arribos de �rdenes y probabilidad de la orden de ser ejecutada. De esta forma, plantean una medidad de probabilidad P con el modelo m�s probable y otra Q con un set de modelos alternativos para agregar robustez frente a la ambiguedad de modelos. Utilizan programaci�n din�mica para encontrar una estrategia �ptima.

%%% Cartea Libro
%Complejizaci�n de Avellaneda - intro
En el cap�tulo 10 de su libro, \cite{Cartea2019a} complejizan lo realizado por Avellaneda con diversas variaciones. 
% problema formal, penalidad de descargar inventario al final, cotas sobre q y risk aversion, resultados similares a avellaneda
En primer lugar, plantean formalmente el problema de \textit{market making} y maximizaci�n de riqueza terminal planteando cotas sobre el inventario m�ximo y un par�metro de aversi�n al riesgo obteniendo resultados similares a los de Avellaneda.
%sin restricciones inventario, maximizar ejecucion
Luego, plantean el problema sin restricciones de inventario obteniendo una soluci�n simple que busca maximizar la ejecuci�n de las �rdenes l�mite.
% at the touch, mercados l�quidos, cruzar order book, DPE
En tercer lugar, definen el problema ``en el tope'' donde el agente debe decidir si colocar ordenes de compra, venta o ambas. Esta caracter�stica refiere a mercados l�quidos donde las �rdenes que no esten en el tope tienen una baja probabilidad de ser ejecutadas y es particularmente reelevante, ya que un \textit{market maker} usualmente buscar� operar en mercados l�quidos.
%optimizaci�n de volumen
Tambi�n plantean una optimizaci�n del volumen de las �rdenes que el agente env�a al libro de �rdnees l�mite.
% como funci�n de utilidad - RL
Si bien Cartea plantea el problema como una maximizaci�n de riqueza terminal, tambi�n analiza su equivalencia como funci�n de utilidad mostrando un paralelo con Avellaneda. Esto es de particular importancia si se analiza la aplicabilidad de estos algoritmos al campo de aprendizaje reforzado donde se busca maximizar una funci�n de utilidad a lo largo del tiempo.
%Selecci�n adversa de dos formas - mid price, alfa signal
Finalmente, ataca el problema de selecci�n adversa, evitado por Avellaneda, de dos maneras: con el impacto en el precio medio causado por las �rdenes de mercado combinando un proceso browninano que replica el flujo de las noticias y considerando la sumatoria de las �rdenes de mercado en el precio; y con un alfa de corto plazo que se integra en el tiempo y es un proceso con reversi�n a la media.

%%% Cartea Alfa Signal
%Inclusi�n de Alpha Signal + impacto MO en midprice, todo en un jump process e inclusi�n de MO para descargar inventario. Muy expl�cito
\cite{Cartea2019} proponen una se�al alfa que modela los efectos de la selecci�n adversa y buscan minimizar sus costos. Esta se�al se ve afectada tanto por las �rdenes de mercado\footnote{El modelo considera la posibilidad de descargar el inventario usando una orden de mercado, por lo que las �rdenes de mercado emitidas por el \textit{market maker} tambi�n generar�n un impacto.} como por \textit{shocks} de difusi�n que representan noticias novedosas. Este modelo es superador en el hecho de que compone los riesgos de selecci�n adversa en proceso de salto, al igual que ocurrir�a en un mercado electr�nico con intervalos discretos; a la vez que plantea que el agente env�a sus �rdenes ``en el tope'' al igual que ocurrir�a en un mercado de alta liquidez y contemplando el riesgo de inventario. Tambi�n se plantea el uso de �rdenes de mercado especulativas para descargar el inventario o tomar una posici�n en caso de que hubiera una se�al alfa lo suficientemente beneficiosa. De esta forma, se atacan varias de las falencias previamente descriptas y se componen los comportamientos deseados. Surge el interrogante, dado que los par�metros fueron estimados en base a informaci�n del NASDAQ si estos modelos son susceptibles de ser utilizados en otros mercados y cual ser�a el desempe�o si se los corriera contra los datos de mercado y no una estimaci�n de par�metros.

\subsection{Aprendizaje reforzado}
%%ML
Los algoritmos de programaci�n din�mica y los de aprendizaje reforzado comparten muchas caracteristicas. Ambos deben maximizar una funci�n de utilidad al finalizar el tiempo t. A su vez, ambos poseen un espacio de estados definido donde pueden actuar. En mucho casos ambos atacan el problema de \textit{market making}. Por esta raz�n, si bien el aprendizaje reforzado no es el foco principal de este trabajo, en esta secci�n se presentan diferentes autores que o bien describen esta t�cnica o atacan este problema con esa t�cnica particular.

\cite{RichardS.Sutton2018} explica las bases del aprendizaje reforzado. Plantea que se trata del aprendizaje desde el error. Hay una serie de elementos comunes como el agente, el ambiente, una pol�tica, una se�al de recompensa, una funci�n de valor y un modelo. 

%Spooner et al.
\cite{Spooner2018} dise�an un simulador que recrea la microestructura del libro de �rdenes en base a los datos hist�ricos de mercado. Dise�an un agente con un espacio de aciones discreto escalado por un \textit{spread} y definen tres funciones de recompensa distintas, incluyendo dos funciones de recompensa moderadas que desincentivan el seguimiento de tendencias y fomentan capturar \textit{spread}. Definen un estado del sistema que incluye el inventario y la microestructura, entre otros.
\begin{comment}
, y utilizan \textit{tile codings}, una version \textit{Linear Combination of Tile Coding(LCTC)}. Utilizan \textit{Q-learning}, SARSA, \textit{R-learning}, \textit{On policy R-learning}, \textit{Double Q-learning}, \textit{Expected SARSA}, \textit{Double R-learning}. Utilizan una serie de agentes denominados simples como \textit{benchmark}(sim�tricos, \textit{random}, RL sin recompensa llamadas moderadas). Se utiliz� un Algoritmo Gen�tico para elegir par�metros. Definenen un \textit{normalized daily PnL(PnL/spread)} y una medida de exposici�n de inventario para evaluar los agentes. 
Se hace un an�lisis de los algoritmos y encuentran que las versiones \textit{on-policy} de los algoritmos funcionan mejor. SARSA funcion� de manera muy consistente. Respecto a las funciones de recompensa, la funci�n moderada sim�trica no funcion� pero la asim�trico usando un factor alto mejor� el retorno ajustado por riesgo y la estabilidad de aprendizaje. Aparentemente el inventario es lo que genera inestabilidad en la funci�n de recompensa. Respecto a los estados, el \textit{tile coding(LCTC)} responde mejor que el uso de \textit{full state}. Finalmente, desarrollan un agente consolidado usando la funci�n de recompensa moderada asim�trica, LCTC y SARSA. Da un \textit{PNL} ligeramente menor pero con mucha mayor estabilidad fuera de muestra y un mejor retorno ajustado por el riesgo, poseyendo mucho menos inventario. Esto dar�a un agente con un mayor comportamiento de market making y menos especulativo.
\end{comment}

%Lim y Gorse
\cite{Lim2018} definen un espacio discreto de estados de inventario, tiempo y precio. Env�an ofertas a cada momento de $t$ con una compesaci�n de {0,1,2} sobre la mejor oferta. Definen dos funciones de recompensa: una en $t$ para capturar las ganancias y riesgo tomados durante la duraci�n de la sesi�n, y una en $T$ para representar la actitud sobre las ganancias intra-diarias y la aversi�n al riesgo al final de la sesi�n de mercado.
\begin{comment}
Realizan simulaciones para comparar la performance de un algoritmos discretos de \textit{Q-learning} contra un \textit{zero tick offset}, el modelo de Avellaneda y un modelo aleatorio. Miden el \textit{profit} y el inventario acumulados. El algoritmo de Aprendizaje Reforzado supera a los otros. Seg�n el nivel de aversi�n al riesgo se modifica el nivel de inventario acumulado al finalizar la sesi�n.
\end{comment}

% Zihao Zhang
% Key findings
\cite{Zhang2019} utilizan algoritmos de aprendizaje reforzado profundo con contratos de futuros. En este caso, no atacan el problema de \textit{market making} sino m�s bien plantean estrategias de inversi�n activas. Escalan sus operaciones por volumen y volatilidad. Realizaron un \textit{backtesting} con 50 contratos. Obtuvienen resultados que mejoran la el rendimiento de estrategias tradicionales.

\cite{Ganesh2019} formalizan el \textit{dealers market} como un sistema multi-agente con M agentes, N inversores y un precio de referencia proveniente de un proceso geom�trico Browniano y crean un simulador. Los agentes ganan \textit{spread} vendiendo a clientes o con \textit{pnl} de Inventario y pueden reducir inventario sesgando sus ofertas o hacer \textit{hedge} a un costo. Definen un agente aleatorio y uno fijo como algoritmos simples. A su vez, formalizan un agente adaptativo que utiliza una tabla de respuesta emp�rica basada en una relaci�n de compromiso de varianza-media entre riesgo y cuota de mercado con una tasa de olvido exponencial.
\begin{comment}
	Utilizan una implementaci�n \textit{standard} de \textit{Proximal Policy Optimization} que se llama Rllib para entrenar un agente de Aprendizaje Reforzado utilizando como funciones de recompensa el \textit{PNL} total y una penalidad(3 propuestas diferentes) relacionada al riesgo de inventario para hacerlo averso al riesgo.  Finalmente, realizan una serie de experimentos analizando \textit{PNL} total, primero sin \textit{drift} de precio, luego con \textit{drift}. Su agente de Aprendizaje Reforzado le gan� a los algoritmos simples; gan� contra el agente adaptativo si este era averso al riesgo pero perdi�/empat� si no. El agente de Aprendizaje Reforzado logr� aprender sobre la pol�tica de precios de sus competidores sin verlos, a hacer \textit{skew} para reducir inventario y a aumentarlo si hay un drift positivo.
\end{comment}

\begin{comment}
	\cite{Briola2021} no hacen \textit{market making}. Utilizan un algoritmo de \textit{Proximal Policy Optimization} que pertence a la familia de los M�todos de \textit{Policy Gradient}. Toman datos del \textit{Limit Order Book} del Nasdaq de una plataforma llamada LOBSTER. Trabajan con INTC y toman 60 d�as de \textit{trading} para el \textit{training set} y 22 para el \textit{test set}. Utilizan 3 modelos para el \textit{training} y el \textit{testing}. Cada uno de ellos difiere en el espacio de estados que se le provee al algoritmo y agrega incrementalmente m�s informaci�n. El primero tiene los vol�menes(de varios niveles del \textit{order book}), �ltimos \textit{ticks}(se incorpora la microestructura de mercado) y posici�n actual(\textit{short}, \textit{long}, \textit{neutral}: solo es posible comprar una unidad del activo), al segundo se le suman \textit{MTM} de la posici�n actual y al tercero el \textit{bid-ask} \textit{spread} actual. El espacio de acciones est� conformado por: \textit{sell}, \textit{stay}, \textit{buy} y \textit{daily\_stop\_loss}(cierre de posici�n y no m�s trading por el d�a para evitar p�rdidas). Se crea un par (posici�n, acci�n) con todas las posibles combinaciones y sus significados. La funci�n de \textit{reward} es una funci�n del par acci�n-estado y es el acumulado del \textit{profit}, exceptuando el stop-loss. El inventario es solo de una unidad. Hay una serie de especificaciones sobre el entrenamiento y testeo de los modelos. Hay set de entrenamiento y de \textit{test}. Se realiza un an�lisis \textit{out-of-sample} de los resultados para los tres sets de estados obteniendose un mayor numero de trades para los modelos m�s complejos, especialmente a causa de que el modelo conozca el \textit{MTM}. No hay una comparaci�n contra un \textit{benchmark}, sea \textit{buy only} o alg�n modelo tradicional de \textit{HFT}.
\end{comment}



%Selser intro
\cite{Selser2021} utilizan t�cnicas de aprendizaje reforzado aprovechando la funci�n de utilidad planteada por \cite{Avellaneda2008} mejorando el perfil de \textit{PnL} para algunos casos. Utilizan varios m�todos de aprendizaje reforzado y comentan sobre la falta de robustez de los algoritmos.
% TODO: faltar�a cr�tica
\begin{comment}
\subsection{Aprendizaje Reforzado Multi-Objetivo}


\cite{Si2017} utilizan \textit{Multi Objective Reinforcement Learning} para crear un algoritmo de \textit{trading}. En este caso se escalariza la funci�n de utilidad (definiendo un valor alfa = 1 y beta = 0.01) convirtiendo el problema multiobjetivo en un problema de objetivo simple. Este ser�a el caso m�s sencillo ya que no se arriba a un set de soluciones sino que se trata en la pr�ctica de un problema de optimizaci�n de una dimensi�n. 

\cite{Hayes2022} realizan una extensa gu�a describiendo casos de uso, la definici�n del problema, \textit{policies} y sets de soluciones, entre otros.
En la definici�n del problema se formaliza proceso de decisi�n de Marvok multi-objetivo, diferenciandose principalmente de su versi�n de objetivo simple en que tiene una funci�n de recompensa Rd siendo d la cantidad de objetivos. Se tienen como en el proceso de Markov de un solo objetivo: espacio de estados(S), espacio de acciones(A), funci�n de transici�n probabilistica(T), factor de descuento y distribuci�n de probabilidad sobre los estados iniciales.
Respecto a las policies y \textit{value functions} se tiene una \textit{policy} $\pi$ que pertenece a $\Pi$ (espacio de \textit{policies}). Pero, su funci�n de valor es un vector $\forall \pi \in Rd$ que surge de calcular la esperanza condicional del vector de recompensas. Normalmente, para obtener la \textit{policy} �ptima se busca la de mayor valor asociado descontado, pero en este caso puede darse que no haya dominancia. Esto se solucionar�a usando una funci�n de escalarizaci�n que vaya del espacio vectorial a los reales. Sin embargo, sin ella solamente se tiene un ordenamiento parcial y no es posible determinar la p�licy �ptima. 
En la secci�n de sets de soluciones se define el set no dominado que arma un frente de Pareto y diferentes estrategias y funciones de utilidad que permiten obtener un subset de soluciones. Incluye sumas escalares de soluciones, \textit{Convex Hull}, etc. Finalmente define el CH($\Pi$) (convex Hull) y el CCS($\Pi$) que son los subsets para las funciones de utilidad lineales.
Se define un enfoque denominado \textit{Utility-based Approach} que en vez de utilizar todo el set de pareto utiliza un subset mucho m�s facil de calcular (m�s bien no imposible computacionalmente). Hay una serie de pasos para obtener el set de soluciones �ptimas en base a la funci�n de utilidad, si es conocida o no y si puede cambiar en el tiempo. 
Hay varios factores que influencian el dise�o del sistema multi-objetivo, tales como desconocer la funci�n de utilidad, un escenario de decisi�n donde las preferencias son dificiles de estimar, otro donde sean conocidas, un escenario interactivo y m�s.
Luego, se pueden definir si se van a utilizar policies multiples o �nicas, funciones de utilidad lineales o monotonicas crecienctes, policies estoc�sticas o determin�sticas y retornos esperados escalarizados o retornos escalarizados esperados.
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section {Descripci�n del problema} \label{sec:problema}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% que es market making
El problema de \textit{market making} consiste en ofrecer permanentemente liquidez en un mercado dado. En su versi�n m�s simple se trata de un activo a intercambiar en el que permanentemente se debe decidir si ofrecer una orden l�mite de compra, de venta o ambas teniendo en cuenta el riesgo que genera el inventario adquirido en la sesi�n de \textit{trading}, el proceso de precios del modelo y la intensidad con la que llegan las �rdenes de mercado. El problema se reduce a un problema de control �ptimo y es por ello que son las t�cnicas de control las aplicadas para resolverlo.

% explicar variaciones, at the top o no, optimizado por volumen, diferentes modelaciones de proceso de precios
Dada esta definici�n existen diversas variaciones a este problema, comenzando por el espacio de estados que el agente puede tomar. En primer lugar, se tiene el caso en el que se define la distancia al precio medio para de esa forma controlar la cantidad de �rdenes llenadas por �rdenes de mercado. Luego, se tiene el caso ``en el tope'' en el que solamente se define si ofrecer o no las �rdenes en el mejor valor posible del libro de �rdenes l�mite. Luego, el modelo tambi�n depender� de la modelizaci�n del proceso de precios subyacente que podr�a ser, por ejemplo, un proceso browniano o un proceso de saltos.

% explicar formas de resoluci�n: con programaci�n din�mica, heur�sticas, RL
Se han utilizado diferentes t�nicas para resolver el problema tales como programaci�n din�mica, diferentes heur�sticas y tambi�n aprendizaje reforzado. La soluci�n a proponer depender� en gran medida del modelo particular que se utilice para entender el problema as� como de la t�cnica elegida.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section {Modelo} \label{sec:modelo} % (Enfoque, modelo, proceso ....) % o modelos, podr�an comentarse los otros modelos implementados.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Se considera el modelo de precios, el problema de optimizaci�n de \textit{market making} y las soluciones todos planteados por \cite{Cartea2019}. En esta secci�n se describe este modelo y sus respectivas soluciones sin variaciones.

El modelo de precios $S_t$ est� regido por

\begin{equation}
	dS_t = \sigma (dJ_t^+ - dJ_t^-),
	\label{dJ}
\end{equation}

donde $S_t$ es un proceso de saltos compuesto por la diferencia entre los procesos $J_t^+$ y $J_t^-$ y cuyo $\sigma$ es el \textit{tick} m�nimo del mercado. La Ecuaci�n \ref{dJ} representa los desbalances de �rdenes de mercado de compra y venta. Siendo $(+)$ las �rdenes de compra que empujan el precio hacia arriba y $(-)$ las �rdenes de venta que empujan el precio hacia abajo.

Cada proceso de salto $J_t^+$($J_t^-$) tiene una media  $\mu_t^+$($\mu_t^-$) estoc�stica definida por

\begin{equation}
	\mu_t^+ = (\alpha_t)_+ + \theta\ \qquad \textit{y} \qquad
	\mu_t^- = (\alpha_t)_- + \theta ,
	\label{mu}
\end{equation}

donde $\theta$ es un par�metro de mercado que define una media f�ja. Esta media se debe a la liquidez propia del mercado que tiene una cantidad de �rdenes de compra y venta de base. A esto se suma la se�al $(\alpha_t)$ que agrega la variabilidad de la media de los procesos $J$. El operador $(\alpha_t)_{+}$ devuelve $\alpha_t$ si $\alpha_t>0$ y si no cero, mientras que el operador $(\alpha_t)_{-}$ devuelve $-\alpha_t$ si $\alpha_t<0$ y si no cero. Esto hace que la se�al $\alpha$ aumente la media del proceso que comparte su signo en determinado $t$.

La se�al $\alpha_t$ depende de la ecuaci�n diferencial estoc�stica

\begin{equation}
	d\alpha_t = -k\alpha_t dt + \xi dW_t + \eta^+ (dM_t^{0+} + dM_t^+) - \eta^- (dM_t^{0-} + dM_t^-), \quad \alpha_0=0
	\label{alpha_dif}
\end{equation}

 donde $k$ es un par�metro de mercado que regula la intensidad de reversi�n a la media del proceso, $\xi$ regula los \textit{shocks} estoc�sticos que representan la noticias novedosas modeladas como un proceso browniano $W_t$. $\eta_+$ es el impacto de las �rdenes de compra de mercado y $\eta_-$ el de las de venta. Los procesos $M_t^{0-}$ y $M_t^{0+}$ representan las �rdenes de mercado de compra y venta que emiten otros participanes del mercado y estan modelados como procesos de conteo que siguen procesos de Poisson con medias $\lambda_+$ y $\lambda_-$. Los procesos $M_t^{+}$ y $M_t^{-}$ son la sumatoria de la cantidad de ejecuciones de las �rdenes de compra y venta ejecutadas por el \textit{market maker} para descargar inventario en tiempo $t$.

El operador $\nu$ define el control del \textit{market maker} y se rige por

\begin{equation}
	\nu = (l^{\pm}, \tau^{\pm}),
	\label{nu}
\end{equation}

donde $l^{\pm}$ expresa a los operadores $l^+$ y $l^-$ que indican si el \textit{market maker} ofrece una orden l�mite de compra y/o de venta; y $\tau_{\pm}$ representa las �rdenes de mercado que el \textit{market maker} utiliza para en tiempos $\tau_+$ y $\tau_-$ para descargar inventario en los casos que sea conveniente.

Se define el inventario controlado por el \textit{market maker} como $Q_t^\nu$. Este surge de tanto las �rdenes l�mite que sean agredidas por otros agentes del mercado como por las �rdenes de mercado que el \textit{market maker} ejecute para descargar inventario. A su vez tiene una cota superior $\overline{Q}$ e inferior $-\overline{Q}$.

La riqueza del agente se define como $X_t^\nu$ y se obtiene en base a las operaciones realizadas en el mercado, ganando el \textit{spread} $\Delta$ en los casos de �rdenes l�mite agredidas; y pagando $\Upsilon$ en los casos que utilice �rdenes de mercado para descargar inventario. El costo $\Upsilon$ equivale al \textit{spread} $\Delta$ sumado a los costos de transacci�n $\epsilon$ de forma que $\Upsilon= \Delta + \epsilon$. 

Finalmente, el problema de optimizaci�n est� definido por

\begin{equation}
	H(t,x,S,\alpha,q) = 
	\sup_{\nu \in \mathcal{A}} \mathbb{E}_{t,x,S,\alpha,q} [X_{T}^\nu + Q_T^\nu(S_T-sign(Q_T^\nu)\Upsilon-\psi Q_T^\nu) - \phi \int_{t}^{T}(Q_s^\nu)^2ds]
	\label{optimizacion}
\end{equation}

donde la funci�n $H$ depende del tiempo $t$, el proceso de riqueza $x$, y la se�al $\alpha$, el proceso de precios $S$ y el inventario $q$. Se busca maximizar la esperanza dentro del espacio de estados $\mathcal{A}$ que puede tomar el control $\nu$. Para ello se necesita maximizar la riqueza terminal $X_T^\nu$ sumado al inventario terminal $Q_T^\nu$ multiplicado por el precio terminal $S_T$ m�s el costo $\Upsilon$ del \textit{spread} y los costos de transacci�n. El par�metro $\psi$ pondera el costo de saltar el libro de �rdenes l�mite y est� en el t�rmino $\psi Q_T^\nu$ que representa los costos de cruzar el libro de �rdenes l�mite al finalizar la sesi�n. Se tiene $\phi$ que representa la aversi�n al riesgo y penaliza cargar el inventario a lo largo de la sesi�n en el t�rmino $- \phi \int_{t}^{T}(Q_s^\nu)^2ds$. La formulaci�n es muy similar a la de \cite{Avellaneda2008}, pero incorpora las diferencias propias de esta definici�n del problema de \textit{market making}.


\cite{Cartea2019} utilizan la siguiente soluci�n de la inecualidad quasi-variacional Hamilton-Jacobi-Bellman 

\begin{gather}
	\text{max} \bigg\{
	\partial_t H
	+ (\alpha^+ + \theta) \big( H(t,x,S+\sigma,\alpha,q) - H \big) \nonumber \\
	+ (\alpha^- + \theta) \big( H(t,x,S-\sigma,\alpha,q) - H \big) \nonumber \\
	-k\alpha \partial_\alpha H + \frac{1}{2} \xi^2 \partial_{\alpha \alpha} H - \phi q^2 \nonumber \\
	+ \lambda^ + \sup_{l_+\in {0,1}} \bigg[l_+ \big( H(t,x + (S+\Delta),S,\alpha+\eta_+,q-1) - H \big) \nonumber \\
	(1-l_+) ( H(t,x,S,\alpha+\eta_+,q) - H )\bigg] \nonumber \\
	+ \lambda^ - \sup_{l_-\in {0,1}} \bigg[l_- \big( H(t,x - (S-\Delta),S,\alpha-\eta_-,q+1) - H \big) \nonumber \\
	(1-l_-) ( H(t,x,S,\alpha-\eta_-,q) - H )\bigg]; \nonumber \\
	H(t,x+(S-\Upsilon),S,\alpha,q-1)-H; \nonumber \\
	H(t,x-(S+\Upsilon),S,\alpha,q+1)-H		 	
\bigg\}=0
\label{HJB}	
\end{gather}

para encontrar un m�todo num�rico que permita obtener una funci�n H. 


Se define como condici�n terminal
\begin{equation}
H(T,x,S,\alpha,q) = x + q (S-\textit{signo}(q)\Upsilon - \psi q)
\label{terminal}
\end{equation}

Los controles estoc�sticos 

\begin{gather}
	\nonumber l_+ = \mathds{1}_{\{H(t,x+(S+\Delta),S,\alpha+\eta^+,q-1)>H(t,x,S,\alpha+\eta^+,q)\}}\\
	l_-=\mathds{1}_{\{H(t,x-(S-\Delta),S,\alpha-\eta^-,q+1)>H(t,x,S,\alpha-\eta^-,q)\}} 
	\label{l}
\end{gather}


donde $l_+$ es el control de venta de orden l�mite y $l_-$ es el control de compra de orden l�mite.


Se plantea este ansatz

\begin{equation}
	H(t,x,S,\alpha,q)=x+qS+\tilde{h}(t,\alpha,q)
	\label{ansatz}
\end{equation}

Obteni�ndose la ecuaci�n

\begin{gather}
	\text{max} \bigg\{
	\partial_t \tilde{h}+\alpha\sigma q-k\alpha \partial_\alpha \tilde{h} + \frac{1}{2} \xi^2 \partial_{\alpha \alpha} \tilde{h} - \phi q^2 \nonumber \\
	+ \lambda^ + \sup_{l_+\in {0,1}} \bigg[l_+ \big(\Delta +  \tilde{h}(t,\alpha+\eta_+,q-1) - \tilde{h} \big) 
	(1-l_+) ( \tilde{h}(t,\alpha+\eta_+,q) - \tilde{h} )\bigg] \nonumber \\
	+ \lambda^ - \sup_{l_-\in {0,1}} \bigg[l_- \big(\Delta + \tilde{h}(t,\alpha-\eta_-,q+1) - \tilde{h} \big) 
	(1-l_-) ( \tilde{h}(t,\alpha-\eta_-,q) - \tilde{h} )\bigg]; \nonumber \\
	\tilde{h}(t,\alpha,q-1)-\tilde{h}; \nonumber \\
	\tilde{h}(t,\alpha,q+1)-\tilde{h}		 	
	\bigg\}=0
	\label{HJB_simple}	
\end{gather}


La condici�n terminal ahora ser�

\begin{equation}
	\tilde{h}(T,\alpha, q) = q \thinspace \text{signo}(q)\Upsilon - \psi q )
	\label{terminal_2}
\end{equation}

donde ahora $\tilde{h}$ solo depende de $t$, $\alpha$ y $q$.

Los controles estoc�sticos finalmente son

\begin{gather}
	\nonumber l_+ = \mathds{1}_{\{\Delta+\tilde{h}(t,\alpha+\eta^+,q-1)>\tilde{h}(t,\alpha+\eta^+,q)\}}\\
	l_-=\mathds{1}_{\{\Delta+\tilde{h}(t,\alpha-\eta^-,q+1)>\tilde{h}(t,\alpha-\eta^-,q)\}} 
	\label{l_2}
\end{gather}

\subsection{Estimaci�n de par�metros}
\cite{Cartea2019} realizan una estimaci�n de m�xima verosimilitud para obtener los par�metros correspondientes a diferentes activos del NASDAQ en base a datos de alta frecuencia de una sesi�n de \textit{trading} de cinco horas y media. Se obtienen los par�metros $\tilde{k}$ de reversi�n a la media, $\tilde{\eta_+}$ y $\tilde{\eta_-}$ de impacto de las �rdenes de mercado a $\alpha$, $\tilde{\theta}$ que es la base de la media que define los procesos $J$, y $\lambda_+$ y $\lambda_-$ que definen la tasa de arribo de los procesos de Poisson de las �rdenes de mercado.

Este mismo m�todo puede ser utilizado para estimar los par�metros de otros mercados como, por ejemplo, el Bovespa.

Tal y como resume \cite{MIURA2011}, el m�todo de m�xima verosimilitud consiste en estimar un par�metro $\theta$ que especifica una funci�n de probabilidad $P(X=x|\theta)$ de una variable estoc�stica discreta $X$ basandose en las observaciones $x_1, x_2 \dots x_n$.

El estimador de m�xima verosimilitud es el valor $\tilde{\theta}$ que maximiza la funci�n de verosimilitud que queda definida por

\begin{equation}
	L(\theta) = \prod_{i=1}^{n}P(X=x_i|\theta)
	\label{verosimilitud}
\end{equation}

De esta forma, se elige el par�metro $\tilde{\theta}$ que sea el m�s verosimil de haber generado los datos.



%\section{Pseudo-c�digo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section {Datos} % (descripcion, fuente, analisis, tratamiento)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section {Metodolog�a}\label{sec:metodo} % (procedimientos)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Se dise�� el siguiente algoritmo que es capaz de seguir el esquema num�rico de \cite{Cartea2019}.
%\begin{figure}

\begin{algorithm}[H]
	\caption{Algoritmo para calcular $\tilde{h}$}
	\begin{algorithmic}[1]
		\Procedure{Calcular $\tilde{h}$}{}
		\For{ $t_i$ (empezando desde $T$ y hacia atr�s)}
			\For {$q_i$}
				\State $\partial_\alpha$$\tilde{h}$ = calcular $\partial_\alpha$$\tilde{h}$ ($h(t_i+1,\forall \alpha,q_i)$)
				\State $\partial_{\alpha \alpha}$$\tilde{h}$ = calcular $\partial_{\alpha \alpha}$$\tilde{h}$ ($h(t_{i+1},\forall \alpha,q_i)$)
				\State ($l_+(t_{i + 1}, \forall \alpha, q_i)$, $l_-(t_{i + 1}, \forall \alpha, q_i)$) = encontrar posiciones optimas$(h, t_i, q_i)$
				\State $h(t_{i},\forall \alpha,q_i)$ = $S_{dt d\alpha}(\tilde{h}, t_i, q_i, \partial_\alpha \tilde{h}, \partial_{\alpha \alpha}\tilde{h})$
			\EndFor
		\EndFor
		\EndProcedure
		\Procedure{$S_{dt d\alpha}$}{$\tilde{h}, t_i, q_i, \partial_\alpha \tilde{h}, \partial_{\alpha \alpha}\tilde{h}$}
		\State $T_{dt d\alpha}$ = $T_{dt d\alpha}(\tilde{h}, t_i, q_i, \partial_\alpha \tilde{h}, \partial_{\alpha \alpha}\tilde{h})$
		\State $M_{dt d\alpha}$ = $T_{dt d\alpha}(\tilde{h}, t_i, q_i)$
		\State \Return max($T_{dt d\alpha}$, $M_{dt d\alpha}$)
		\EndProcedure
	\end{algorithmic}
\label{algoritmoH}	
\end{algorithm}

%\end{figure}

El Algoritmo \ref{algoritmoH} converge a una funci�n $\tilde{h}$. Se utilizaron como referencia el c�digo de \cite{JaimungalCodigo} que muestra la generaci�n de ciertas figuras para el libro de \cite{Cartea2019a} y obtiene una funci�n $\tilde{h}$ para otro dise�o de problema, y el c�digo de \cite{KHelertCode} que replica algunas otras figuras del mismo libro.

\cite{Cartea2019} definen un esquema num�rico de que se implement� en c�digo y define una funci�n $S_{dt d\alpha}$ que permite obtener la funci�n �ptima de forma incremental. $S_{dt d\alpha}$ devuelve el m�ximo entre otras dos funciones $T_{dt d\alpha}$ que busca el valor �ptimo de $\tilde{h}$ optimizando el control que el \textit{market maker} realiza sobre las �rdenes l�mite y $M_{dt d\alpha}$ que har� lo mismo sobre las �rdenes de mercado que podr�an ser utilizadas para descargar inventario.

Finalmente, se defini� un algoritmo de simulaci�n que permite a cada instante del tiempo obtener los posicionamientos �ptimos $l_+$, $l_-$, $\tau_+$ y $\tau_-$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section {Resultados preliminares} \label{sec:resultados}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Simulaciones}

El objetivo principal de esta propuesta fue replicar los resultados obtenidos por \cite{Cartea2019} en su modelo con se�al alfa en el tope del libro de �rdenes. Estos resultados, y el modelo programado con el que se obtuvieron, son la base fundamental para aplicar este modelo a datos de mercados emergentes.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\linewidth]{figuras/h_final}
	\caption{Funci�n h.}
	\label{fig:h}
\end{figure}

Se reprodujo la funci�n $\tilde{h}$ en la Figura \ref{fig:h}. Se logr� resolviendo la ecuaci�n diferencial \ref{HJB_simple} de forma num�rica siguiendo el m�todo num�rico planteado tambi�n por \cite{Cartea2019}. La funci�n $\tilde{h}$ determina en qu� momentos deben ofrecerse operaciones de compra, venta o ambas, mediante el uso de las funciones $l_+$ y $l_-$ descriptas en la Ecuaci�n \ref{l_2}. A simple vista se puede ver c�mo la funci�n $\tilde{h}$ en el sector de $\alpha=-300$ crece en la direcci�n de $-q$ con m�xima pendiente. Esto ocurre porque cuando $\alpha$ tiene un valor muy negativo indica que el desbalance entre oferta y demanda es fuerte en la direcci�n de baja de precio. De esta forma, el agente aumentar� su utilidad en mayor medida cuanto m�s inventario negativo tenga, o cuanto m�s reduzca el inventario actual. Ocurre lo opuesto en el caso de $\alpha=300$, donde lo conveniente ser� aumentar el inventario. La funci�n es suave en direcci�n a los casos intermedios.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/positioning_vs_q_final}
	\caption{Comportamientos del agente seg�n $\tilde{h}$ en funci�n de t para q=0.}
	\label{fig:positioningvsq}
	%Arriba: fucsia
	%medio arriba: verde
	%medio: amarillo
	%medio abajo: rosa
	%abajo: azul
	%final: rojo
\end{figure}

Componiendo las ecuaciones $l_+$ y $l_-$ de la Ecuaci�n \ref{l_2} con la funci�n $\tilde{h}$ se obtiene el comportamiento esperado del agente para un caso dado. En la figura \ref{fig:positioningvsq}\footnote{A continuaci�n los comportamientos del agente seg�n el color. Fucsia: ofrecer orden l�mite y de mercado para comprar inventario. Gris: ofrecer �rden l�mite para compra inventario. Amarillo: ofrecer orden l�mite para compar y para vender inventario. Rosa: ofrecer orden l�mite para vender inventario. Azul: ofrecer orden de mercado y l�mite para vender inventario. Rojo: no ofrecer ni �rdenes de mercado ni l�mite.} se fij� $q=0$ de forma tal de entender este comportamiento. El agente toma diferentes posiciones dependiendo del momento en la sesi�n y la fuerza de la se�al $\alpha$. En los casos en los que la se�al es baja y falta mucho para terminar la sesi�n el agente opta por ofrecer ambas puntas de forma tal de maximizar el intercambio. Si la se�al $\alpha$ sube o baja sobrepasando alrededor del valor 50 el agente comienza a sesgar sus compras en la direcci�n que indica la se�al. En el caso en que la se�al sea lo suficientemente fuerte, como por ejemplo en $t=50$ y $\alpha=\pm 200$ adem�s el agente toma posiciones especulativas ofreciendo una oferta de mercado. Como es esperable, dado que la funci�n de utilidad penaliza saltar el libro de �rdenes l�mite al finalizar la sesi�n, cuando se acerca el final de la sesi�n el agente intenta no ejecutar �rdenes si su inventario es cero o intenta descargar inventario en caso de tenerlo.

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=1\linewidth]{figuras/limit_orders_minus_executions_final}
		\caption{�rdenes de compra.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
	\includegraphics[width=1\linewidth]{figuras/limit_orders_plus_executions_final}
	\caption{�rdenes de venta.}
\end{subfigure}
	\caption{Histograma de �rdenes l�mite ejecutadas.}
	\label{fig:limitordersminusplusexecutions}
\end{figure}

Uno de los par�metros considerados que \cite{Cartea2019} muestran en su trabajo es el perfil de �rdenes l�mite y de mercado ejecutadas. En la Figura \ref{fig:limitordersminusplusexecutions} se realiz� un histograma de la ejecuci�n de �rdenes de compra y venta de las �rdenes l�mite en base a 10 simulaciones utilizando el c�digo desarrollado. Los resultados son compatibles con aquellos que se buscaban replicar.
\begin{figure}[H]
	\centering
\begin{subfigure}{0.45\textwidth}
	\includegraphics[width=1\linewidth]{figuras/market_orders_minus_executions_final}
	\caption{�rdenes de compra.}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\textwidth}
	\includegraphics[width=1\linewidth]{figuras/market_orders_plus_executions_final}
	\caption{�rdenes de venta.}
\end{subfigure}
	\caption{Histograma de �rdenes de mercado ejecutadas.}
	\label{fig:marketordersplusexecutions}
\end{figure}

Por otro lado, tambi�n se realiz� un histograma de las �rdenes de mercado realizadas en la Figura \ref{fig:marketordersplusexecutions}. El agente utiliza estas �rdenes para descargar inventario en casos donde la se�al $\alpha$ sea lo suficientemente alta como para pagar el costo de saltar el libro de �rdenes o para tomar posiciones especultivas en los casos en que la se�al sea muy alta y se quiera aprovechar para obtener una ganancia.

\begin{figure}[H]
\begin{subfigure}{0.8\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{figuras/orders_final}
	\caption{Trayectoria del precio y las �rdenes del agente.Linea negra: precio medio. L�nea roja: ofrecimiento de �rdenes de compra l�mite. L�nea azul: ofrecimiento de �rdenes de venta l�mite.  Cruz roja: ejecucui�n de �rdenes de compra l�mite. Cruz azul: ejecuci�n de �rdenes de venta l�mite. Cuadrados rojo y azul: ejecuci�n de �rdenes de compra y venta de mercado.}
	\label{fig:orders}
\end{subfigure}
\begin{subfigure}{0.8\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{figuras/pnl_final}
	\caption{\textit{PnL}.}
	\label{fig:pnl}
\end{subfigure}
\begin{subfigure}{0.8\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{figuras/q_final}
	\caption{q.}
	\label{fig:q}
\end{subfigure}
\begin{subfigure}{0.8\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{figuras/alpha_final}
	\caption{Alfa.}
	\label{fig:alpha}
\end{subfigure}
\caption{Ejemplo de simulaci�n.}
\label{fig:orders_pnl_q_alfa}
\end{figure}

Finalmente, se grafic� una simulaci�n de ejemplo en la Figura \ref{fig:orders_pnl_q_alfa}. En la subfigura \ref{fig:orders} se graficaron el precio medio, las puntas que ofrece el agente y los eventos de ejecuci�n de �rdenes l�mite y de mercado. En la subfigura \ref{fig:pnl} se grafica el retorno del agente a lo largo de la sesi�n. En la subfigura \ref{fig:q} se observa el inventario. En la subfigura \ref{fig:alpha} se grafica la se�al $\alpha$.

A lo largo de la simulaci�n se observa que desde el inicio el agente ofrece ambas puntas: compradora y vendedora. Alrededor de $t=25$ se observa que no se ofrece m�s punta vendedora por haber llegado al m�ximo de inventario negativo por un breve lapso de tiempo. Se observa el mismo comportamiento en $t=40$. En $t=45$, ocurre la misma situaci�n en 3 ocasiones. Alrededor de $t=57$ el agente deja de ofrecer �rdenes de compra dado que llega a su m�ximo inventario de compra. En lo que va de la sesi�n hasta ese momento el agente ha podido aprovechar las �rdenes que llegan en ambos sentidos con excepci�n de estos breves lapsos de tiempo. Estando muy cerca de finalizar la sesi�n, el agente deja de ofertar �rdenes de compra aunque tiene capacidad en su inventario para adquirir m�s del activo. Esto quiere decir que el agente se sesga hacia reducir el inventario por la proximidad a finalizar la sesi�n y las condiciones de la se�al $\alpha$. Finalmente, realiza una �rden de mercado para descargar el inventario remanente.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section {Plan de trabajo futuro} \label{sec: futuro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Los pasos a seguir para concluir el trabajo son los siguientes:
\begin{itemize}
	\item Se definir� una estrategia de referencia que no tome en consideraci�n la se�al alfa para evaluar en cu�nto mejora el desempe�o del modelo propuesto respecto a este modelo base.
	\item Se buscar� obtener los par�metros correspondientes a un mercado latinoamericano\footnote{Se utilizar� el m�todo de m�xima verosimilitud para obtener los par�metros que maximicen el criterio.}, por ejemplo BOVESPA, para medir el desempe�o del modelo en un mercado emergente. Por otro lado se comparar�n estos par�metros y sus resultados contra los resultados ya obtenidos en base a los par�metros del NASDAQ. 
	\item Se realizar� una simulaci�n del modelo utilizando los datos reales. Es decir, se buscar� ya no generar una simulaci�n en base a los par�metros, sino testear el modelo contra datos de una sesi�n de \textit{trading}.
\end{itemize}

%\addcontentsline{toc}{section}{References}
\bibliography{biblio}


\begin{appendices}
\section{C�digo} \label{sec:codigo}
\subsection{Par�metros}
\begin{lstlisting}[language=Python]
from types import SimpleNamespace
simulation_parameters = {
	'q_max': 4,
	'T': 60,
	'A': 300,
	'dalpha': 30,
	'Delta': 0.005,
	'epsilon': 0.005,
	'psi': 0.01,
	'phi_': 1e-6,
	'eta': 60.0,
	'sigma': 0.01,
	'k': 200.0,
	'xi': 1.0,
	'lambda_plus': 1.0,
	'lambda_minus': 1.0,
	'theta': 0.1,
	's0': 100,
	'n': 10
}
p = SimpleNamespace(**simulation_parameters)
p.dt = (p.k * p.A / p.dalpha + p.lambda_plus + p.lambda_minus)**(-1)
\end{lstlisting}
\subsection{Definiciones}
\begin{lstlisting}[language=Python]
import numpy as np

q_max, T, A, dalpha, Delta, epsilon, psi, phi_, eta, sigma, k,
 xi, lambda_plus, lambda_minus = p.q_max, p.T, p.A, p.dalpha,
p.Delta, p.epsilon,  p.psi, p.phi_, p.eta, p.sigma, p.k, p.xi,
p.lambda_plus, p.lambda_minus

Upsilon = Delta + epsilon

dt = (k * A / dalpha + lambda_plus + lambda_minus)**(-1)

q_a = np.arange(-q_max, q_max + 1, 1)
alpha = np.arange(-A, A + 1, dalpha)

alpha_smaller_0 = np.where(alpha < 0)[0]
alpha_greater_0 = np.where(alpha > 0)[0]
alpha_0 = np.where(alpha == 0)[0]   

n_q = len(q_a)
n_alpha = len(alpha)
n_t = int(T / dt)

h = np.full((n_t, n_alpha, n_q), np.nan)
d_alpha_h = np.zeros(n_alpha)
dd_alpha_h = np.zeros(n_alpha)

l_plus = np.zeros((n_t, n_alpha, n_q))
l_minus = np.zeros((n_t, n_alpha, n_q))

h_eta_up = np.full((n_t, n_alpha, n_q), np.nan)
h_eta_down = np.full((n_t, n_alpha, n_q), np.nan)

def T_dt_dalpha(h, t_i, q_i, d_alpha_h, dd_alpha_h):
    h_t_1_q = h[t_i + 1, :, q_i]
    q_ = q_a[q_i]

    l_plus_term = get_l_plus_term(t_i, q_i, h_t_1_q)

    l_minus_term = get_l_minus_term(t_i, q_i, h_t_1_q)

    h_t_q = h_t_1_q + dt * (
        alpha * sigma * q_
        - k * alpha * d_alpha_h
        + ((xi**2) / 2) * dd_alpha_h
        - phi_ * q_**2
        + l_plus_term
        + l_minus_term
    )

    h_t_q[0] = 2 * h_t_q[1] - h_t_q[2]
    h_t_q[-1] = 2 * h_t_q[-2] - h_t_q[-3]
    return h_t_q

def get_l_minus_term(t_i, q_i, h_t_1_q):
    if q_a[q_i] < q_max:
        l_minus_term = lambda_minus * np.maximum(
            (Delta + h_eta_down[t_i + 1, :, q_i + 1] - h_t_1_q),
            (h_eta_down[t_i + 1, :, q_i] - h_t_1_q),
        )
    else:
        l_minus_term = h_eta_down[t_i + 1, :, q_i] - h_t_1_q
    return l_minus_term


def get_l_plus_term(t_i, q_i, h_t_1_q):
    if q_a[q_i] > -q_max:
        l_plus_term = lambda_plus * np.maximum(
            (Delta + h_eta_up[t_i + 1, :, q_i - 1] - h_t_1_q),
            (h_eta_up[t_i + 1, :, q_i] - h_t_1_q),
        )
    else:
        l_plus_term = h_eta_up[t_i + 1, :, q_i] - h_t_1_q
    return l_plus_term


def M_dt_dalpha(h, t_i, q_i):
    if q_a[q_i] < q_max and q_a[q_i] > -q_max:
        return np.maximum(
            (h[t_i + 1, :, q_i - 1] - Upsilon), (h[t_i + 1, :, q_i + 1] - Upsilon)
        )
    elif q_a[q_i] > -q_max:
        return h[t_i + 1, :, q_i - 1] - Upsilon
    elif q_a[q_i] < q_max:
        return h[t_i + 1, :, q_i + 1] - Upsilon
    else:
        raise ValueError(f"Imposible Case {q_a[q_i]}")


def S_dt_dalpha(h, t_i, q_i, d_alpha_h, dd_alpha_h):
    T_dt_dalpha_i = T_dt_dalpha(h, t_i, q_i, d_alpha_h, dd_alpha_h)
    M_dt_dalpha_i = M_dt_dalpha(h, t_i, q_i)
    return np.maximum(T_dt_dalpha_i, M_dt_dalpha_i)


def calculate_d_alpha_h(h_q_t):
    d_alpha_h[alpha_smaller_0] = (
        h_q_t[alpha_smaller_0 + 1] - h_q_t[alpha_smaller_0]
    ) / dalpha
    d_alpha_h[alpha_greater_0] = (
        h_q_t[alpha_greater_0] - h_q_t[alpha_greater_0 - 1]
    ) / dalpha
    d_alpha_h[alpha_0] = (
        (h_q_t[alpha_0 + 1] - h_q_t[alpha_0]) +
        (h_q_t[alpha_0] - h_q_t[alpha_0 - 1])
    ) / (2 * dalpha)
    return d_alpha_h


def calculate_dd_alpha_h(h_q_t):
    dd_alpha_h[1:-1] = (h_q_t[2:] - 2 * h_q_t[1:-1] - h_q_t[:-2]) / (dalpha**2)
    return dd_alpha_h


def extrapolate_up(phi, n, diff):
    delta_phi = phi[-1] - phi[-2]
    phi_extrapolated = (
        np.ones(n) * phi[-1] + diff * delta_phi + np.arange(0, n) * delta_phi
    )
    return phi_extrapolated


def interpolate(phi, up=True):
    eta_dalpha = eta / dalpha
    eta_dalpha_floor = np.floor(eta_dalpha)
    eta_dalpha_diff = eta_dalpha - eta_dalpha_floor
    eta_move = int(eta_dalpha_floor)

    phi_eta = phi if up else np.flip(phi)

    phi_eta = np.roll(phi_eta, -eta_move)
    phi_eta[-eta_move:] = np.nan

    phi_eta_1 = np.roll(phi_eta, -1)
    phi_eta_1[-1:] = np.nan

    phi_eta += (phi_eta_1 - phi_eta) * eta_dalpha_diff
    phi_eta[-eta_move - 1:] = extrapolate_up(
        phi if up else np.flip(phi), len(
            phi_eta[-eta_move - 1:]), eta_dalpha_diff
    )

    phi_eta = phi_eta if up else np.flip(phi_eta)

    return phi_eta


def find_optimal_postings(h, t_i, q_i):
    h_eta_up[t_i + 1, :, q_i] = interpolate(h[t_i + 1, :, q_i])
    if q_a[q_i] > -q_max:
        h_eta_up[t_i + 1, :, q_i - 1] = interpolate(h[t_i + 1, :, q_i - 1])
        l_plus_i = np.where(
            Delta + h_eta_up[t_i + 1, :, q_i -
                             1] > h_eta_up[t_i + 1, :, q_i], 1, 0
        )
    else:
        l_plus_i = np.zeros(n_alpha)

    h_eta_down[t_i + 1, :, q_i] = interpolate(h[t_i + 1, :, q_i], up=False)
    if q_a[q_i] < q_max:
        h_eta_down[t_i + 1, :, q_i +
                   1] = interpolate(h[t_i + 1, :, q_i + 1], up=False)
        l_minus_i = np.where(
            Delta + h_eta_down[t_i + 1, :, q_i +
                               1] > h_eta_down[t_i + 1, :, q_i], 1, 0
        )
    else:
        l_minus_i = np.zeros(n_alpha)
    return l_plus_i, l_minus_i
   
\end{lstlisting}
\subsection{C�lculo de h}
\begin{lstlisting}[language=Python]
h[-1, :, :] = (
    np.ones((1, n_alpha)) *
    np.array([(q_a * (-np.sign(q_a) * Upsilon - psi * q_a))]).T
).T

for t_i in range(n_t - 2, -1, -1):
    for q_i in range(n_q):
        h_q_t_1 = h[t_i + 1, :, q_i]
        d_alpha_h = calculate_d_alpha_h(h_q_t_1)
        dd_alpha_h = calculate_dd_alpha_h(h_q_t_1)
        l_plus[t_i + 1, :, q_i], l_minus[t_i + 1, :, q_i] = 
        	find_optimal_postings(
            h, t_i, q_i
        )
        h[t_i, :, q_i] = S_dt_dalpha(h, t_i, q_i, d_alpha_h, dd_alpha_h)
\end{lstlisting}
\subsection{Obtenci�n de �rdenes de mercado �ptimas}
\begin{lstlisting}[language=Python]
def find_optimal_MO(h, t_i, q_i):
    if q_a[q_i] > -(q_max - 1):
        mo_minus_i = np.where((
        	h[t_i + 1, :, q_i - 1] - Upsilon) > h[t_i + 1, :, q_i], 1, 0)
    else:
        mo_minus_i = np.zeros(n_alpha)

    if q_a[q_i] < (q_max - 1):
        mo_plus_i = np.where(
        (h[t_i + 1, :, q_i + 1] - Upsilon) > h[t_i + 1, :, q_i],1,0)
    else:
        mo_plus_i = np.zeros(n_alpha)


    return mo_plus_i, mo_minus_i

mo_plus = np.zeros((n_t, n_alpha, n_q))
mo_minus = np.zeros((n_t, n_alpha, n_q))

for t_i in range(n_t - 2, -1, -1):
    for q_i in range(n_q):
        mo_plus[t_i + 1, :, q_i], mo_minus[
        	t_i + 1, :, q_i] = find_optimal_MO(
            h, t_i, q_i
        )
\end{lstlisting}
\subsection{Simulaciones}
\begin{lstlisting}[language=Python]
import numpy as np
h = np.load("h.npy")
q = np.load("q.npy")
alpha = np.load("alpha.npy")
l_plus = np.load("l_plus.npy")
l_minus = np.load("l_minus.npy")
mo_plus = np.load("mo_plus.npy")
mo_minus = np.load("mo_minus.npy")

from matplotlib import pyplot as plt

np.random.seed(1)
dMt_minus = 0
dMt_plus = 0


def generate_simulations(p, h, l_p, l_m, mo_p, mo_m, plot=False):
    n, k, eta_plus, eta_minus, lambda_plus, lambda_minus,
    T, xi, sigma, theta, s0, A, dalpha, q_max, Delta, epsilon = p.n, 
    p.k, p.eta, p.eta, p.lambda_plus, p.lambda_minus, p.T, p.xi,
    p.sigma, p.theta, p.s0, p.A, p.dalpha, p.q_max, p.Delta, p.epsilon

    Upsilon = Delta + epsilon

    dt = (k * A / dalpha + lambda_plus + lambda_minus)**(-1)
    
    m = int(T/dt)
    
    # Alpha setup
    alpha = np.full((n, m), np.nan)
    alpha[:, 0] = 0
    alpha_range = np.arange(-A, A + 1, dalpha)

    tau_plus_amounts = np.random.poisson(lambda_plus*T, n)
    tau_minus_amounts = np.random.poisson(lambda_minus*T, n)
    tau_plus = [np.sort(np.random.rand(
    	tau_i) * T) for tau_i in tau_plus_amounts]
    tau_minus = [np.sort(np.random.rand(
    	tau_i) * T) for tau_i in tau_minus_amounts]

    dMt0_plus = np.array(
    	[np.histogram(tau_i,np.linspace(0,T,m+1))[0] for tau_i in tau_plus])
    dMt0_minus = np.array(
    	[np.histogram(tau_i,np.linspace(0,T,m+1))[0] for tau_i in tau_minus])

    # S setup
    s = np.full((n, m), np.nan)
    s[:, 0] = s0

    mu_plus = np.full((n, m), np.nan)
    mu_plus[:, 0] = theta
    mu_minus = np.full((n, m), np.nan)
    mu_minus[:, 0] = theta

    dJ_plus = np.full((n, m), np.nan)
    dJ_plus[:, 0] = 0

    dJ_minus = np.full((n, m), np.nan)
    dJ_minus[:, 0] = 0

    # Positions setup
    l_p_position = np.full((n, m), np.nan)
    l_m_position = np.full((n, m), np.nan)

    p_postings = np.full((n, m), np.nan)
    m_postings = np.full((n, m), np.nan)

    p_executions = np.full((n, m), np.nan)
    m_executions = np.full((n, m), np.nan)

    p_executions_count = np.full((n, m), np.nan)
    m_executions_count = np.full((n, m), np.nan)

    mo_p_executions = np.full((n, m), np.nan)
    mo_m_executions = np.full((n, m), np.nan)

    dMt_plus = np.full((n, m), np.nan) # np.zeros((n, m))
    dMt_minus = np.full((n, m), np.nan) # np.zeros((n, m))

    pnl = np.full((n, m), np.nan)
    pnl[:, 0] = 0

    X = np.full((n, m), np.nan)
    X[:, 0] = 0

    def get_closest_index(val):
        return int(np.round(min(max(
        	-p.A,val),p.A) / p.dalpha, 0)) + int(p.A / p.dalpha)

    def get_l_p(t_i, alpha_val, q):
        alpha_i = get_closest_index(alpha_val)
        q_i = int(q + q_max)
        return l_p[t_i, alpha_i, q_i]
    get_l_p_v = np.vectorize(get_l_p)

    def get_l_m(t_i, alpha_val, q):
        alpha_i = get_closest_index(alpha_val)
        q_i = int(q + q_max)
        return l_m[t_i, alpha_i, q_i]
    get_l_m_v = np.vectorize(get_l_m)

    def get_MM_MO_p(t_i, alpha_val, q):
        alpha_i = get_closest_index(alpha_val)
        q_i = int(q + q_max)
        return mo_p[t_i, alpha_i, q_i]
    get_MM_MO_p_v = np.vectorize(get_MM_MO_p)
    
    def get_MM_MO_m(t_i, alpha_val, q):
        alpha_i = get_closest_index(alpha_val)
        q_i = int(q + q_max)
        return mo_m[t_i, alpha_i, q_i]
    get_MM_MO_m_v = np.vectorize(get_MM_MO_m)

    # Inventory setup
    q = np.full((n, m), np.nan)
    q[:, 0] = 0

    # Simulations
    for i in range(m-1):
        #dMt_minus and dMt_plus depend on the MM
        dMt_plus[:, i] = get_MM_MO_p_v(i, alpha[:, i], q[:, i])
        dMt_minus[:, i] = get_MM_MO_m_v(i, alpha[:, i], q[:, i])

        l_p_position[:, i] = get_l_p_v(i, alpha[:, i], q[:, i])
        l_m_position[:, i] = get_l_m_v(i, alpha[:, i], q[:, i])

        alpha[:, i+1] = alpha[:,i] * np.exp(-k * dt) + xi * np.sqrt(
        	dt) * (np.random.randn(n)) + eta_plus *(
        	dMt0_plus[:,i] + dMt_plus[:, i]) - eta_minus * (
        	dMt0_minus[:,i] + dMt_minus[:, i])

        mu_plus[:, i+1] = np.where(alpha[:, i+1]>0, alpha[:, i+1],0) + theta
        mu_minus[:, i+1] = np.where(alpha[:, i+1]<0, -alpha[:, i+1],0) + theta

        dJ_plus[:, i+1] = np.where(np.random.rand(n) < np.around(
        	(1 - np.exp(-dt * (mu_plus[:,i+1]))), decimals=4),1,0)
        dJ_minus[:, i+1] = np.where(np.random.rand(n) < np.around(
        	(1 - np.exp(-dt * (mu_minus[:,i+1]))), decimals=4),1,0)
        
        s[:,i+1] = s[:,i] + sigma * (dJ_plus[:, i+1] - dJ_minus[:, i+1])

        q[:, i+1] = q[:, i] 
        	- np.where(l_p_position[:, i] * dMt0_plus[:, i] > 0,1,0) 
        	+ np.where((l_m_position[:, i] * dMt0_minus[:, i]) > 0,1,0)
        	- np.where(dMt_minus[:, i] > 0,1,0) 
        	+ np.where(dMt_plus[:,i] > 0,1,0)

        p_postings[:, i] = np.where(
        	l_p_position[:,i]==0, np.nan, (s[:,i]+Delta)*l_p_position[:,i])
        p_executions_count[:,i] = np.where(
        	l_p_position[:,i]*dMt0_plus[:,i]==0, 0, 1)
        p_executions[:, i] = np.where(l_p_position[:,i]*dMt0_plus[:,i]==0, np.nan, (s[:,i]+Delta)*l_p_position[:,i]*np.where(dMt0_plus[:,i]>0,1,0))
        
        m_postings[:,i] = np.where(
        	l_m_position[:,i]==0, np.nan, (s[:,i]-Delta)*l_m_position[:,i])
        m_executions_count[:,i] = np.where(
        	l_m_position[:,i]*dMt0_minus[:,i]==0, 0, 1)
        m_executions[:,i] = np.where(
        	l_m_position[:,i]*dMt0_minus[:,i]==0, np.nan, (s[:,i]-Delta)*l_m_position[:,i]*np.where(dMt0_minus[:,i]>0,1,0))

        mo_p_executions[:,i] = np.where(
        	dMt_plus[:, i]==0, np.nan, (s[:,i]+Upsilon)*dMt_plus[:, i])
        mo_m_executions[:,i] = np.where(
        	dMt_minus[:, i]==0, np.nan, (s[:,i]-Upsilon)*dMt_minus[:, i])

        X[:,i+1] = X[:,i] 
        	+ np.where(p_executions[:,i+1] > 0, s[:, i+1] + Delta, 0) \
        	- np.where(m_executions[:,i+1] > 0, s[:, i+1]-Delta, 0)\
            - np.where(mo_p_executions[:,i+1] > 0, s[:, i+1] + Upsilon, 0) \
            + np.where(mo_m_executions[:,i+1] > 0, s[:, i+1] - Upsilon, 0)

        pnl[:,i+1] = pnl[:,i] 
        	+ np.where(p_executions[:,i] > 0, Delta, 0) \
        	+ np.where(m_executions[:,i] > 0, Delta, 0)\
            + q[:, i] * (s[:, i+1] - s[:, i]) \
            - np.where(mo_p_executions[:,i+1] > 0, Upsilon, 0) \
            - np.where(mo_m_executions[:,i+1] > 0, Upsilon, 0)
        
    X[:,-1] = X[:,-1] - q[:, -1] * (s[:, -1]) - np.abs(q[:,-1])*Upsilon

    if plot:
        plt_i = 1
        plt.figure(figsize=(25,7))
        plt.title('Alpha')
        plt.step(np.linspace(0,T,m),alpha[plt_i])

        plt.figure(figsize=(25,7))
        plt.title('S')
        plt.step(np.linspace(0,T,m), s[plt_i], c='black')
        
        plt.step(np.linspace(0,T,m), p_postings[plt_i], c='b')
        plt.scatter(np.linspace(0,T,m), p_executions[plt_i], marker='x', c='b')

        plt.step(np.linspace(0,T,m), m_postings[plt_i], c='r')
        plt.scatter(np.linspace(0,T,m), m_executions[plt_i], marker='x', c='r')

        plt.scatter(np.linspace(0,T,m), mo_m_executions[plt_i], marker='s', c='b')
        plt.scatter(np.linspace(0,T,m), mo_p_executions[plt_i], marker='s', c='r')
        print(f"MO_p: {np.nansum(dMt_plus[plt_i])}")
        print(f"MO_m: {np.nansum(dMt_minus[plt_i])}")
        print(f"LO_p: {np.nansum(m_executions_count[plt_i])}")
        print(f"LO_m: {np.nansum(p_executions_count[plt_i])}")
        print(f"Mean of PNL:{np.average(pnl[:,-1])}")
        print(f"Stde of PNL:{np.std(pnl[:,-1])}")
        print(f"Mean of X:{np.average(X[:,-1])}")
        print(f"Stde of X:{np.std(X[:,-1])}")

        plt.figure()
        plt.title('Limit Orders Minus Executions')
        plt.hist(m_executions_count[:,:-1].sum(axis=1))
        
        plt.figure()
        plt.title('Limit Orders Plus Executions')
        plt.hist(p_executions_count[:,:-1].sum(axis=1))

        plt.figure()
        plt.title('Market Orders Minus Executions')
        plt.hist(dMt_minus[:, :-1].sum(axis=1))
        
        plt.figure()
        plt.title('Market Orders Plus Executions')
        plt.hist(dMt_plus[:, :-1].sum(axis=1))

        if False:
            plt.figure()
            plt.title('$\mu_+$')
            plt.step(np.linspace(0,T,m),mu_plus[plt_i])

            plt.figure()
            plt.title('$\mu_-$')
            plt.step(np.linspace(0,T,m),mu_minus[plt_i])
        
        plt.figure(figsize=(25,7))
        plt.title('$q$')
        plt.step(np.linspace(0,T,m),q[plt_i])

        plt.figure(figsize=(25,7))
        plt.title('$pnl$')
        plt.step(np.linspace(0,T,m),pnl[plt_i])
        
        
    return alpha, mu_plus, mu_minus, dJ_plus, dJ_minus, \
    	s, l_p_position, l_m_position, q, dMt0_plus,\
    	dMt0_minus, pnl, dMt_plus, dMt_minus, \
    	p_executions_count, m_executions_count, pnl, X

np.random.seed(2)

alpha, mu_plus, mu_minus, dJ_plus, dJ_minus, s, 
l_p_position, l_m_position, q, dMt0_plus, dMt0_minus,
pnl, dMt_plus, dMt_minus, p_executions_count, m_executions_count, pnl, X = 
generate_simulations(p, h, l_plus, l_minus, mo_plus, mo_minus, plot=True)
\end{lstlisting}
\end{appendices}


\end{document}

